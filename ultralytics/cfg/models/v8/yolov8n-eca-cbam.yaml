# YOLOv8n with ECA + CBAM Multi-Attention
# ============================================================================
# Experiment: YOLOv8n with sequential ECA → CBAM multi-attention
# Purpose: Combine ECA efficiency with CBAM comprehensive attention

# Parameters
nc: 6 # number of classes (HRIPCB dataset)
scales: # model compound scaling constants
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024] # YOLOv8n summary: 225 layers, 3157200 parameters, 8.9 GFLOPs

# YOLOv8n backbone with ECA+CBAM multi-attention
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f_ECA_CBAM, [256, True]]  # 4 - Multi-attention integration
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f_ECA_CBAM, [512, True]]  # 6 - Multi-attention integration  
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f_ECA_CBAM, [1024, True]]  # 8 - Multi-attention integration
  - [-1, 1, SPPF, [1024, 5]]  # 9

# YOLOv8n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12

  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f, [256]]  # 15 (P3/8-small)

  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f, [512]]  # 18 (P4/16-medium)

  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f, [1024]]  # 21 (P5/32-large)

  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)

# Model metadata
metadata:
  architecture: "YOLOv8n-ECA-CBAM"
  attention_mechanism: "Sequential ECA → CBAM"
  base_model: "YOLOv8n"
  placement_strategy: "strategic_backbone_layers"
  target_layers: [4, 6, 8]
  description: "YOLOv8n with progressive ECA+CBAM multi-attention"
  
  # Multi-attention configuration
  multi_attention_config:
    type: "sequential"
    pipeline: ["ECA", "CBAM"]
    layers: [4, 6, 8]
    channels: [256, 512, 1024]
    dropout: 0.05
    total_parameters: "+1K-10K parameters"
    computational_overhead: "+9-13% FLOPs"
    
  # Performance expectations
  performance:
    base_yolov8n: "baseline performance"
    expected_improvement: "+6-8% mAP"
    efficiency: "balanced cost vs performance"
    inference_impact: "-10-15% speed"

# Design rationale
rationale:
  eca_benefits: |
    ECA provides initial efficient channel focus:
    1. Minimal computational overhead (<1% FLOPs)
    2. Adaptive kernel size selection
    3. Quick channel importance identification
    4. Prepares features for CBAM refinement
  
  cbam_benefits: |
    CBAM adds comprehensive attention refinement:
    1. Sequential channel and spatial attention
    2. Detailed defect pattern recognition
    3. Spatial localization enhancement
    4. Proven effectiveness in object detection
    
  combination_rationale: |
    Sequential ECA→CBAM provides:
    1. Progressive attention refinement
    2. Balanced computational cost
    3. Complementary attention mechanisms
    4. Optimal for PCB defect detection

# Compatibility notes  
compatibility:
  pretrained_weights: "yolov8n.pt (with adaptation)"
  training_strategy: "standard_training"
  export_formats: ["ONNX", "TensorRT", "CoreML", "TorchScript"]
  deployment_platforms: ["CPU", "GPU", "Edge_Devices"]