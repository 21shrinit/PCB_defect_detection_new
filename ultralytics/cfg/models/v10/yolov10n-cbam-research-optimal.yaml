# YOLOv10n with CBAM Attention - Research-Optimal Placement
# ============================================================================
# Research-backed CBAM placement for comprehensive channel + spatial attention
# Strategy: Hybrid backbone-neck placement with SCDown compatibility

# Parameters
nc: 6 # number of classes (HRIPCB dataset)
scales: # model compound scaling constants
  n: [0.33, 0.25, 1024]

# YOLOv10n backbone with research-optimal CBAM placement
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]           # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]          # 1-P2/4
  - [-1, 3, C2f_CBAM, [128, True]]      # 2 - First attention point (early features)
  - [-1, 1, Conv, [256, 3, 2]]          # 3-P3/8
  - [-1, 6, C2f_CBAM, [256, True]]      # 4 - Second attention point (P3 features)
  - [-1, 1, SCDown, [512, 3, 2]]        # 5-P4/16 (Compatible with CBAM)
  - [-1, 6, C2f_CBAM, [512, True]]      # 6 - Third attention point (post-SCDown P4)
  - [-1, 1, SCDown, [1024, 3, 2]]       # 7-P5/32 (Second SCDown)
  - [-1, 3, C2f_CBAM, [1024, True]]     # 8 - Fourth attention point (post-SCDown P5)
  - [-1, 1, SPPF, [1024, 5]]            # 9 - Standard SPPF
  - [-1, 1, PSA, [1024]]                # 10 - Keep YOLOv10n PSA (CBAM local + PSA global)

# YOLOv10n head with strategic CBAM neck placement
head:
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
  - [[-1, 6], 1, Concat, [1]]            # cat backbone P4
  - [-1, 3, C2f_CBAM, [512]]             # 13 - Neck attention (feature fusion)

  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
  - [[-1, 4], 1, Concat, [1]]            # cat backbone P3
  - [-1, 3, C2f_CBAM, [256]]             # 16 - P3 attention (P3/8-small objects)

  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 13], 1, Concat, [1]]           # cat head P4
  - [-1, 3, C2f_CBAM, [512]]             # 19 - P4 attention (P4/16-medium objects)

  - [-1, 1, SCDown, [512, 3, 2]]         # YOLOv10n SCDown in head
  - [[-1, 10], 1, Concat, [1]]           # cat head P5
  - [-1, 3, C2fCIB, [1024, True, True]]  # 22 - Keep C2fCIB standard (P5/32-large objects)

  - [[16, 19, 22], 1, v10Detect, [nc]]   # YOLOv10n Detect(P3, P4, P5)

# Model metadata
metadata:
  architecture: "YOLOv10n-CBAM-Research-Optimal"
  attention_mechanism: "CBAM"
  base_model: "YOLOv10n"
  placement_strategy: "hybrid_backbone_neck_comprehensive"
  research_basis: "multi_scale_channel_spatial_attention"
  description: "YOLOv10n with CBAM at positions 2,4,6,8,13,16,19 for comprehensive attention coverage"
  
  # Performance expectations based on research
  expected_improvements:
    map50: "+2.5-4.0% over YOLOv10n baseline"
    parameter_overhead: "2-5%"
    flops_overhead: "3-8%"
    training_stability: "good"
    
  # Architectural justification
  design_rationale:
    scdown_compatibility: "CBAM preserves spatial-channel relationships like SCDown"
    psa_complementarity: "CBAM local multi-modal attention complements PSA global context"
    multi_scale_enhancement: "Channel + spatial attention at each resolution level"
    computational_balance: "CBAM moderate overhead distributed across network layers"