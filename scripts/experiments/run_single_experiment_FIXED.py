#!/usr/bin/env python3
"""
FIXED Single Experiment Runner - Integration Complete
====================================================

This is a FIXED version that properly handles:
- Advanced loss functions (focal_siou, verifocal_eiou, etc.)
- Loss weights (box_weight, cls_weight, dfl_weight)
- All training parameters from configs
- Model validation and verification

Features FIXED:
‚úÖ Loss function integration
‚úÖ Loss weight application  
‚úÖ Complete parameter passing
‚úÖ Model loading validation
‚úÖ YOLOv10n/YOLOv11n support
"""

import os
import sys
import yaml
import argparse
import logging
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Union

# Add project root to path for imports FIRST
PROJECT_ROOT = Path(__file__).parent.parent.parent  # Go up from scripts/experiments/
sys.path.insert(0, str(PROJECT_ROOT))

# Ensure ultralytics uses local version with custom modules
ultralytics_path = PROJECT_ROOT / "ultralytics"
if ultralytics_path.exists():
    print(f"Using local ultralytics with custom modules: {ultralytics_path}")

# Third-party imports (after path setup)
import torch
from ultralytics import YOLO
from ultralytics.utils import LOGGER

# WandB import with error handling
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    wandb = None

def setup_logging(experiment_name: str):
    """Setup logging for the experiment."""
    log_filename = f"experiment_{experiment_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(log_filename)
        ]
    )
    return logging.getLogger(__name__)


class FixedExperimentRunner:
    """
    FIXED experiment runner with complete integration.
    
    ‚úÖ Handles all loss functions
    ‚úÖ Applies loss weights
    ‚úÖ Complete parameter integration
    ‚úÖ Model validation
    """
    
    def __init__(self, config_path: str):
        """Initialize the FIXED experiment runner."""
        self.config_path = Path(config_path)
        self.config = self.load_config()
        self.experiment_info = self.extract_experiment_info()
        
        # Setup logging
        self.logger = setup_logging(self.experiment_info['name'])
        
        # Initialize experiment tracking
        self.results = {
            'experiment_info': self.experiment_info,
            'training_results': None,
            'validation_results': None,
            'test_results': None,
            'model_path': None,
            'export_results': None,
            'timing': {},
            'metrics_summary': {}
        }
        
        self.logger.info("‚úÖ FIXED ExperimentRunner initialized")
        self.logger.info(f"Config: {self.config_path}")
        self.logger.info(f"Experiment: {self.experiment_info['name']}")
        
    def load_config(self) -> Dict[str, Any]:
        """Load and validate configuration file."""
        try:
            if not self.config_path.exists():
                raise FileNotFoundError(f"Config file not found: {self.config_path}")
                
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                
            # Validate required sections
            required_sections = ['experiment', 'model', 'data', 'training']
            missing_sections = [section for section in required_sections if section not in config]
            
            if missing_sections:
                raise ValueError(f"Missing required config sections: {missing_sections}")
                
            return config
            
        except Exception as e:
            print(f"‚ùå Error loading config: {e}")
            sys.exit(1)
            
    def extract_experiment_info(self) -> Dict[str, Any]:
        """Extract experiment metadata."""
        exp_config = self.config['experiment']
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        return {
            'name': exp_config.get('name', f'experiment_{timestamp}'),
            'type': exp_config.get('type', 'detection'),
            'description': exp_config.get('description', 'PCB defect detection experiment'),
            'tags': exp_config.get('tags', []),
            'timestamp': timestamp,
            'config_path': str(self.config_path),
            'model_type': self.config['model'].get('type', 'yolov8n'),
            'attention_mechanism': self.config['model'].get('attention_mechanism', 'none'),
            'image_size': self.config['training'].get('imgsz', 640)
        }

    def setup_wandb(self):
        """Setup WandB integration."""
        if not WANDB_AVAILABLE:
            self.logger.info("WandB not available, skipping integration")
            return
            
        try:
            if 'wandb' not in self.config:
                self.logger.info("No W&B config found, skipping integration")
                return
                
            wandb_config = self.config['wandb']
            
            # Set environment variables for ultralytics integration
            os.environ['WANDB_NAME'] = self.experiment_info['name']
            os.environ['WANDB_NOTES'] = self.experiment_info['description']
            
            if self.experiment_info['tags']:
                os.environ['WANDB_TAGS'] = ','.join(self.experiment_info['tags'])
            
            self.logger.info(f"‚úÖ WandB configured: {wandb_config.get('project')}")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è  WandB setup failed: {e}")

    def validate_model_loading(self, model: YOLO, model_config: dict):
        """‚úÖ FIXED: Validate model loading and architecture."""
        try:
            self.logger.info("üîç Validating model loading...")
            
            model_type = model_config.get('type', 'yolov8n')
            attention_mechanism = model_config.get('attention_mechanism', 'none')
            
            # Validate model exists
            if model is None:
                raise ValueError("Model failed to load")
            
            # Validate model type
            self.logger.info(f"‚úÖ Model type: {model_type}")
            self.logger.info(f"‚úÖ Attention mechanism: {attention_mechanism}")
            
            # Check for custom architectures
            if model_config.get('config_path'):
                self.logger.info(f"‚úÖ Custom architecture loaded: {model_config['config_path']}")
                
                # Validate attention modules are available
                if attention_mechanism in ['cbam', 'eca', 'coordatt']:
                    try:
                        if attention_mechanism == 'cbam':
                            from ultralytics.nn.modules.block import C2f_CBAM
                            self.logger.info("‚úÖ C2f_CBAM module verified")
                        elif attention_mechanism == 'eca':
                            from ultralytics.nn.modules.block import C2f_ECA
                            self.logger.info("‚úÖ C2f_ECA module verified")
                        elif attention_mechanism == 'coordatt':
                            from ultralytics.nn.modules.block import C2f_CoordAtt
                            self.logger.info("‚úÖ C2f_CoordAtt module verified")
                    except ImportError as e:
                        self.logger.error(f"‚ùå Attention module import failed: {e}")
                        raise
            
            self.logger.info("‚úÖ Model validation passed")
            
        except Exception as e:
            self.logger.error(f"‚ùå Model validation failed: {e}")
            raise

    def create_model(self) -> YOLO:
        """‚úÖ FIXED: Create YOLO model with validation."""
        try:
            model_config = self.config['model']
            model_type = model_config.get('type', 'yolov8n')
            
            self.logger.info(f"üèóÔ∏è  Creating model: {model_type}")
            
            # Handle custom model configurations
            if model_config.get('config_path'):
                model_path = model_config['config_path']
                if not os.path.exists(model_path):
                    # Try relative to project root
                    model_path = PROJECT_ROOT / model_path
                
                if os.path.exists(model_path):
                    self.logger.info(f"Loading custom model from: {model_path}")
                    model = YOLO(str(model_path))
                else:
                    self.logger.warning(f"Custom model not found at {model_path}, using pretrained: {model_type}")
                    model = YOLO(f'{model_type}.pt')
            else:
                # Use pretrained model for baseline configs
                model = YOLO(f'{model_type}.pt')
                self.logger.info(f"Pretrained model loaded: {model_type}")
            
            # ‚úÖ FIXED: Validate model loading
            self.validate_model_loading(model, model_config)
                
            return model
            
        except Exception as e:
            self.logger.error(f"Model creation failed: {e}")
            import traceback
            self.logger.error(f"Full traceback: {traceback.format_exc()}")
            raise

    def apply_loss_configuration(self, model: YOLO, train_args: dict, training_config: dict):
        """‚úÖ FIXED: Apply loss function configuration."""
        try:
            loss_config = training_config.get('loss', {})
            loss_type = loss_config.get('type', 'standard')
            
            self.logger.info(f"üéØ Configuring loss function: {loss_type}")
            
            # ‚úÖ FIXED: Apply loss weights
            if 'box_weight' in loss_config:
                train_args['box'] = loss_config['box_weight']
                self.logger.info(f"   ‚úÖ Box weight: {loss_config['box_weight']}")
                
            if 'cls_weight' in loss_config:
                train_args['cls'] = loss_config['cls_weight']
                self.logger.info(f"   ‚úÖ Classification weight: {loss_config['cls_weight']}")
                
            if 'dfl_weight' in loss_config:
                train_args['dfl'] = loss_config['dfl_weight']
                self.logger.info(f"   ‚úÖ DFL weight: {loss_config['dfl_weight']}")
            
            # ‚úÖ FIXED: Advanced loss function integration (COMPLETED)
            # Map loss configuration to IoU and classification types
            iou_type = 'ciou'  # default
            cls_type = 'bce'   # default
            
            if loss_type in ['siou', 'eiou', 'ciou', 'giou']:
                iou_type = loss_type
                self.logger.info(f"   ‚úÖ IoU loss type: {iou_type}")
            elif loss_type in ['focal_siou', 'focal_eiou', 'focal_ciou', 'focal_giou']:
                iou_type = loss_type.split('_')[1]  # extract IoU type
                cls_type = 'focal'
                self.logger.info(f"   ‚úÖ IoU loss type: {iou_type}")
                self.logger.info(f"   ‚úÖ Classification loss type: {cls_type}")
            elif loss_type in ['verifocal_siou', 'verifocal_eiou', 'verifocal_ciou', 'verifocal_giou']:
                iou_type = loss_type.split('_')[1]  # extract IoU type  
                cls_type = 'varifocal'
                self.logger.info(f"   ‚úÖ IoU loss type: {iou_type}")
                self.logger.info(f"   ‚úÖ Classification loss type: {cls_type}")
            elif loss_type == 'focal':
                cls_type = 'focal'
                # iou_type remains default CIoU
                self.logger.info(f"   ‚úÖ Classification loss type: {cls_type}")
                self.logger.info(f"   ‚úÖ IoU loss type: {iou_type} (default)")
            elif loss_type == 'varifocal':
                cls_type = 'varifocal'
                # iou_type remains default CIoU
                self.logger.info(f"   ‚úÖ Classification loss type: {cls_type}")
                self.logger.info(f"   ‚úÖ IoU loss type: {iou_type} (default)")
                
            # ‚úÖ FIXED: Set loss types on model.model.args using SimpleNamespace (proper way per tasks.py)
            # init_criterion() uses getattr(self.args, 'iou_type', 'ciou') which requires attribute access
            from types import SimpleNamespace
            
            # Preserve existing args and convert to SimpleNamespace for attribute access
            if hasattr(model.model, 'args') and model.model.args:
                existing_args = dict(model.model.args) if isinstance(model.model.args, dict) else model.model.args.__dict__
            else:
                existing_args = {}
            
            # Add loss configuration
            existing_args['iou_type'] = iou_type
            existing_args['cls_type'] = cls_type
            
            # Convert to SimpleNamespace for getattr() access in init_criterion()
            model.model.args = SimpleNamespace(**existing_args)
            
            self.logger.info(f"‚úÖ Set model.model.args: iou_type={iou_type}, cls_type={cls_type}")
            self.logger.info("   Converted to SimpleNamespace for DetectionModel.init_criterion() compatibility")
            
            # Note: These are NOT passed as training arguments - they're model configuration
            
            if loss_type != 'standard':
                self.logger.info(f"‚úÖ IMPLEMENTED: Advanced loss type '{loss_type}' fully integrated")
                self.logger.info("   Complete loss function integration with configurable IoU and classification losses")
                
        except Exception as e:
            self.logger.error(f"‚ùå Loss configuration failed: {e}")
            raise

    def run_training(self, model: YOLO) -> Dict[str, Any]:
        """‚úÖ FIXED: Execute model training with complete parameter integration."""
        try:
            self.logger.info("üèãÔ∏è  Starting FIXED training phase...")
            training_config = self.config['training']
            data_config = self.config['data']
            
            # ‚úÖ FIXED: Prepare comprehensive training arguments
            # Handle both data.path and training.dataset.path structures
            if 'path' in data_config:
                data_path = data_config['path']
            elif 'dataset' in training_config and 'path' in training_config['dataset']:
                data_path = training_config['dataset']['path']
            else:
                raise ValueError("Dataset path not found in config. Expected data.path or training.dataset.path")
            
            train_args = {
                'data': data_path,
                'epochs': training_config.get('epochs', 100),
                'imgsz': training_config.get('imgsz', 640),
                'batch': training_config.get('batch', 16),
                'device': training_config.get('device', 'auto'),
                'workers': training_config.get('workers', 8),
                'cache': training_config.get('cache', True),
                'amp': training_config.get('amp', True),
                'project': training_config.get('project', 'experiments/pcb-defect-150epochs-v1'),
                'name': self.experiment_info['name'],
                'exist_ok': True,
                'save_period': training_config.get('save_period', 50),
                'val': training_config.get('validate', True)
            }
            
            # ‚úÖ FIXED: Add pretrained weights handling
            model_config = self.config['model']
            if model_config.get('pretrained', False) and model_config.get('config_path'):
                model_type = model_config.get('type', 'yolov8n')
                train_args['pretrained'] = f'{model_type}.pt'
                self.logger.info(f"üéØ Using pretrained weights: {train_args['pretrained']}")
            
            # ‚úÖ FIXED: Add ALL optimizer settings
            optimizer_params = ['optimizer', 'lr0', 'lrf', 'weight_decay', 'momentum', 'warmup_epochs', 'patience']
            for param in optimizer_params:
                if param in training_config:
                    train_args[param] = training_config[param]
                    self.logger.info(f"   ‚úÖ {param}: {training_config[param]}")
            
            # ‚úÖ FIXED: Add cosine learning rate support
            if training_config.get('cos_lr'):
                train_args['cos_lr'] = True
                self.logger.info("   ‚úÖ Cosine learning rate enabled")
                
            # ‚úÖ FIXED: Add ALL augmentation settings
            if 'augmentation' in training_config:
                aug_config = training_config['augmentation']
                aug_params = ['mosaic', 'mixup', 'copy_paste', 'hsv_h', 'hsv_s', 'hsv_v', 
                             'degrees', 'translate', 'scale', 'shear', 'perspective', 'flipud', 'fliplr']
                for param in aug_params:
                    if param in aug_config:
                        train_args[param] = aug_config[param]
            
            # ‚úÖ FIXED: Apply loss configuration with model parameter
            self.apply_loss_configuration(model, train_args, training_config)
            
            self.logger.info("‚úÖ FIXED Training configuration (COMPLETE):")
            for key, value in train_args.items():
                self.logger.info(f"   {key}: {value}")
            
            # Execute training
            start_time = time.time()
            results = model.train(**train_args)
            training_time = time.time() - start_time
            
            self.logger.info(f"‚úÖ Training completed in {training_time:.2f} seconds")
            
            # Store results
            training_results = {
                'duration': training_time,
                'best_fitness': getattr(results, 'best_fitness', None),
                'results_dir': getattr(results, 'save_dir', None),
                'best_model_path': str(getattr(results, 'save_dir', '')) + '/weights/best.pt' if hasattr(results, 'save_dir') else None,
                'train_args': train_args
            }
            
            return training_results
            
        except Exception as e:
            self.logger.error(f"‚ùå Training failed: {e}")
            import traceback
            self.logger.error(f"Full traceback: {traceback.format_exc()}")
            raise

    def run_complete_experiment(self):
        """‚úÖ FIXED: Run complete experiment with validation."""
        try:
            self.logger.info("üöÄ Starting FIXED complete experiment...")
            
            # Setup WandB
            self.setup_wandb()
            
            # Create and validate model
            model = self.create_model()
            
            # Run training with complete integration
            training_results = self.run_training(model)
            
            self.logger.info("‚úÖ FIXED experiment completed successfully!")
            
            # Return results in expected format for comprehensive runner
            return {
                'status': 'completed',
                'training_results': training_results,
                'best_model_path': training_results.get('best_model_path'),
                'experiment_info': self.experiment_info,
                'config_path': str(self.config_path)
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå FIXED experiment failed: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'experiment_info': self.experiment_info,
                'config_path': str(self.config_path)
            }


def main():
    """Main entry point for FIXED experiment runner."""
    parser = argparse.ArgumentParser(description='FIXED Single Experiment Runner')
    parser.add_argument('--config', required=True, help='Path to experiment config YAML')
    parser.add_argument('--test_only', action='store_true', help='Run testing only')
    
    args = parser.parse_args()
    
    try:
        runner = FixedExperimentRunner(args.config)
        results = runner.run_complete_experiment()
        print("‚úÖ FIXED experiment completed successfully!")
        
    except Exception as e:
        print(f"‚ùå FIXED experiment failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()