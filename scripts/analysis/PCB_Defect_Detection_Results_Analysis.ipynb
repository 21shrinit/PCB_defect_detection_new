{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCB Defect Detection: Systematic Study Results Analysis\n",
    "\n",
    "## Comprehensive Analysis of YOLOv8 Variants, Attention Mechanisms, and Loss Functions\n",
    "\n",
    "This notebook provides automated analysis of experimental results from Weights & Biases, focusing on:\n",
    "- **Performance Comparison**: mAP@0.5, mAP@0.5-0.95, precision, recall, F1 score\n",
    "- **Efficiency Analysis**: Training time, inference speed, model parameters\n",
    "- **Trade-off Visualization**: Pareto plots for edge deployment optimization\n",
    "- **Model Selection**: Optimal configurations for different deployment scenarios\n",
    "\n",
    "---\n",
    "\n",
    "**Experiment Overview:**\n",
    "- **Phase 1**: Baseline model comparison (YOLOv8n, YOLOv8s, YOLOv10s)\n",
    "- **Phase 2**: Attention mechanisms (ECA, CBAM, CoordAtt)\n",
    "- **Phase 3**: Loss functions and high-resolution studies\n",
    "\n",
    "**Dataset**: HRIPCB (6 PCB defect classes)\n",
    "**Hardware**: 22GB GPU, 50GB RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport getpass\nfrom PIL import Image\nimport io\nimport base64\n\n# WandB API with authentication\ntry:\n    import wandb\n    print(\"âœ… WandB imported successfully\")\n    WANDB_AVAILABLE = True\nexcept ImportError:\n    print(\"âŒ WandB not found. Install with: !pip install wandb\")\n    WANDB_AVAILABLE = False\n\n# Statistical analysis\ntry:\n    from scipy import stats\n    from scipy.spatial.distance import pdist, squareform\n    print(\"âœ… SciPy imported successfully\")\nexcept ImportError:\n    print(\"âŒ SciPy not found. Install with: !pip install scipy\")\n\n# Advanced plotting\ntry:\n    import plotly.express as px\n    import plotly.graph_objects as go\n    from plotly.subplots import make_subplots\n    import plotly.offline as pyo\n    pyo.init_notebook_mode(connected=True)\n    print(\"âœ… Plotly imported successfully\")\n    PLOTLY_AVAILABLE = True\nexcept ImportError:\n    print(\"âš ï¸  Plotly not available. Install with: !pip install plotly\")\n    PLOTLY_AVAILABLE = False\n\n# Configure matplotlib\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['axes.titlesize'] = 14\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['legend.fontsize'] = 10\n\n# Configure seaborn\nsns.set_palette(\"husl\")\n\nprint(\"\\nðŸš€ Setup complete! Ready for analysis.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# WandB Configuration\nWANDB_PROJECT = \"pcb-defect-150epochs-v1\"\nWANDB_ENTITY = None  # Set to your username if needed\n\n# Authentication settings\nWANDB_API_KEY = None  # Will be set during authentication\nWANDB_AUTHENTICATED = False\n\n# Analysis Configuration\nSAVE_PLOTS = True\nOUTPUT_DIR = Path(\"analysis_results\")\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Edge Deployment Constraints\nEDGE_CONSTRAINTS = {\n    'max_model_size_mb': 50,      # Maximum model size for edge deployment\n    'min_fps': 10,                # Minimum inference speed requirement\n    'min_map50': 0.60,           # Minimum acceptable mAP@0.5\n    'max_inference_time_ms': 100  # Maximum inference time per image\n}\n\n# Color scheme for consistency\nMODEL_COLORS = {\n    'yolov8n': '#1f77b4',\n    'yolov8s': '#ff7f0e', \n    'yolov10s': '#2ca02c'\n}\n\nATTENTION_COLORS = {\n    'none': '#1f77b4',\n    'eca': '#ff7f0e',\n    'cbam': '#2ca02c',\n    'coordatt': '#d62728'\n}\n\nprint(f\"ðŸ“Š Project: {WANDB_PROJECT}\")\nprint(f\"ðŸ“ Output directory: {OUTPUT_DIR}\")\nprint(f\"âš¡ Edge constraints: {EDGE_CONSTRAINTS}\")\n\n# Display authentication status\nprint(f\"ðŸ” WandB Available: {WANDB_AVAILABLE}\")\nif WANDB_AVAILABLE:\n    print(f\"ðŸ” Authentication Status: {'âœ… Ready' if WANDB_AUTHENTICATED else 'âŒ Required'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "source": "def fetch_wandb_runs(project_name, entity=None):\n    \"\"\"\n    Fetch all runs from WandB project and extract comprehensive metrics with enhanced validation.\n    \n    Returns:\n        pd.DataFrame: Comprehensive experiment results\n    \"\"\"\n    if not WANDB_AUTHENTICATED:\n        print(\"âŒ WandB not authenticated. Please run authentication first.\")\n        return pd.DataFrame()\n    \n    print(\"ðŸ”„ Fetching runs from WandB with comprehensive data validation...\")\n    \n    try:\n        # Initialize WandB API\n        api = wandb.Api()\n        \n        # Get project path\n        project_path = f\"{entity}/{project_name}\" if entity else project_name\n        runs = api.runs(project_path)\n        \n        print(f\"ðŸ“¥ Found {len(runs)} total runs\")\n        \n        # Filter for finished runs only\n        finished_runs = [run for run in runs if run.state == 'finished']\n        print(f\"âœ… Found {len(finished_runs)} finished runs for analysis\")\n        \n        if len(finished_runs) == 0:\n            print(\"âŒ No finished runs found. Cannot proceed with analysis.\")\n            return pd.DataFrame()\n        \n        # Extract run data with enhanced error handling\n        runs_data = []\n        failed_runs = []\n        \n        for i, run in enumerate(finished_runs):\n            if i % 5 == 0:  # Progress indicator\n                print(f\"   Processing run {i+1}/{len(finished_runs)}...\")\n                \n            try:\n                # Basic run information\n                run_data = {\n                    'run_id': run.id,\n                    'run_name': run.name,\n                    'state': run.state,\n                    'created_at': run.created_at,\n                    'duration_seconds': run._attrs.get('runtime', 0),\n                    'tags': run.tags,\n                    'notes': run.notes,\n                    'url': run.url\n                }\n                \n                # Extract configuration with validation\n                config = run.config if hasattr(run, 'config') and run.config else {}\n                \n                # Model configuration\n                model_config = config.get('model_config', {})\n                run_data.update({\n                    'model_type': model_config.get('type', 'unknown'),\n                    'attention_mechanism': model_config.get('attention_mechanism', 'none'),\n                    'model_config_path': model_config.get('config_path', ''),\n                    'pretrained': model_config.get('pretrained', True)\n                })\n                \n                # Training configuration  \n                training_config = config.get('training_config', {})\n                run_data.update({\n                    'batch_size': training_config.get('batch', 'unknown'),\n                    'image_size': training_config.get('imgsz', 640),\n                    'epochs': training_config.get('epochs', 'unknown'),\n                    'optimizer': training_config.get('optimizer', 'unknown'),\n                    'learning_rate': training_config.get('lr0', 'unknown'),\n                    'weight_decay': training_config.get('weight_decay', 'unknown'),\n                    'amp': training_config.get('amp', True),\n                    'cache': training_config.get('cache', True)\n                })\n                \n                # Loss configuration\n                loss_config = training_config.get('loss', {})\n                run_data.update({\n                    'loss_type': loss_config.get('type', 'standard'),\n                    'box_weight': loss_config.get('box_weight', 7.5),\n                    'cls_weight': loss_config.get('cls_weight', 0.5),\n                    'dfl_weight': loss_config.get('dfl_weight', 1.5)\n                })\n                \n                # Data configuration\n                data_config = config.get('data_config', {})\n                run_data['num_classes'] = data_config.get('num_classes', 6)\n                \n                # Extract final metrics from summary with validation\n                summary = run.summary if hasattr(run, 'summary') and run.summary else {}\n                \n                # Performance metrics with multiple possible keys\n                metrics_mapping = {\n                    'map50': ['metrics/mAP50', 'metrics/mAP50(B)', 'val/mAP50', 'mAP50'],\n                    'map50_95': ['metrics/mAP50-95', 'metrics/mAP50-95(B)', 'val/mAP50-95', 'mAP50-95'],\n                    'precision': ['metrics/precision(B)', 'metrics/precision', 'val/precision', 'precision'],\n                    'recall': ['metrics/recall(B)', 'metrics/recall', 'val/recall', 'recall'],\n                    'f1_score': ['metrics/F1(B)', 'metrics/F1', 'val/F1', 'f1'],\n                    'box_loss': ['train/box_loss', 'box_loss'],\n                    'cls_loss': ['train/cls_loss', 'cls_loss'], \n                    'dfl_loss': ['train/dfl_loss', 'dfl_loss'],\n                    'val_box_loss': ['val/box_loss'],\n                    'val_cls_loss': ['val/cls_loss'],\n                    'val_dfl_loss': ['val/dfl_loss'],\n                    'total_parameters': ['model/total_parameters'],\n                    'trainable_parameters': ['model/trainable_parameters'],\n                    'training_time_seconds': ['final/training_time_seconds', 'training_time']\n                }\n                \n                # Extract metrics with fallback and validation\n                metrics_found = 0\n                for metric_name, possible_keys in metrics_mapping.items():\n                    value = None\n                    for key in possible_keys:\n                        if key in summary and summary[key] is not None:\n                            value = summary[key]\n                            metrics_found += 1\n                            break\n                    run_data[metric_name] = value\n                \n                # Calculate GFLOPs\n                if run_data.get('model_type') and run_data.get('image_size'):\n                    gflops = calculate_gflops(run_data['model_type'], run_data['image_size'])\n                    run_data['gflops'] = gflops\n                else:\n                    run_data['gflops'] = None\n                    \n                # Speed metrics (if available)\n                if 'speed/inference' in summary:\n                    run_data['inference_time_ms'] = summary['speed/inference']\n                    run_data['fps'] = 1000.0 / summary['speed/inference'] if summary['speed/inference'] > 0 else None\n                elif 'speed' in summary and isinstance(summary['speed'], dict):\n                    speed_dict = summary['speed']\n                    if 'inference' in speed_dict:\n                        run_data['inference_time_ms'] = speed_dict['inference']\n                        run_data['fps'] = 1000.0 / speed_dict['inference'] if speed_dict['inference'] > 0 else None\n                        \n                # Per-class metrics (HRIPCB has 6 classes)\n                class_names = ['Missing_hole', 'Mouse_bite', 'Open_circuit', 'Short', 'Spurious_copper', 'Spur']\n                for i, class_name in enumerate(class_names):\n                    class_map_key = f'metrics/mAP50(B)_{i}'\n                    if class_map_key in summary:\n                        run_data[f'{class_name}_mAP50'] = summary[class_map_key]\n                        \n                # Extract phase and experiment type from tags\n                phase = 'Unknown'\n                experiment_type = 'Unknown'\n                \n                for tag in run.tags:\n                    if 'phase_1' in tag:\n                        phase = 'Phase 1: Baselines'\n                    elif 'phase_2' in tag:\n                        phase = 'Phase 2: Attention'\n                    elif 'phase_3' in tag:\n                        phase = 'Phase 3: Loss/Resolution'\n                        \n                    if 'model_scaling' in tag:\n                        experiment_type = 'Model Scaling'\n                    elif 'attention_study' in tag:\n                        experiment_type = 'Attention Study'\n                    elif 'architecture_study' in tag:\n                        experiment_type = 'Architecture Study'\n                    elif 'resolution_study' in tag:\n                        experiment_type = 'Resolution Study'\n                        \n                run_data['phase'] = phase\n                run_data['experiment_type'] = experiment_type\n                \n                # Data quality check\n                if metrics_found < 3:  # At least 3 key metrics should be present\n                    print(f\"   âš ï¸  Run {run.name}: Only {metrics_found} metrics found\")\n                \n                runs_data.append(run_data)\n                \n            except Exception as e:\n                print(f\"   âŒ Error processing run {run.name}: {e}\")\n                failed_runs.append(run.name)\n                continue\n        \n        # Create DataFrame\n        df = pd.DataFrame(runs_data)\n        \n        if len(df) == 0:\n            print(\"âŒ No valid run data extracted\")\n            return pd.DataFrame()\n        \n        # Data cleaning and preprocessing\n        df = clean_enhanced_dataframe(df)\n        \n        # Report results\n        print(f\"\\\\nðŸ“Š DATA EXTRACTION SUMMARY:\")\n        print(f\"   âœ… Successfully processed: {len(df)} runs\")\n        print(f\"   âŒ Failed to process: {len(failed_runs)} runs\")\n        \n        if failed_runs:\n            print(f\"   Failed runs: {', '.join(failed_runs[:5])}{'...' if len(failed_runs) > 5 else ''}\")\n        \n        # Data quality report\n        key_metrics = ['map50', 'map50_95', 'precision', 'recall', 'f1_score', 'gflops']\n        print(f\"\\\\nðŸ” DATA QUALITY REPORT:\")\n        for metric in key_metrics:\n            if metric in df.columns:\n                valid_count = df[metric].notna().sum()\n                percentage = (valid_count / len(df)) * 100\n                status = \"âœ…\" if percentage >= 80 else \"âš ï¸ \" if percentage >= 50 else \"âŒ\"\n                print(f\"   {status} {metric}: {valid_count}/{len(df)} ({percentage:.1f}%)\")\n        \n        return df\n        \n    except Exception as e:\n        print(f\"âŒ Critical error during data fetching: {e}\")\n        return pd.DataFrame()\n\n\ndef clean_enhanced_dataframe(df):\n    \"\"\"\n    Enhanced data cleaning and preprocessing with better validation.\n    \"\"\"\n    print(\"ðŸ§¹ Cleaning and preprocessing data...\")\n    \n    # Convert numeric columns with better error handling\n    numeric_columns = [\n        'map50', 'map50_95', 'precision', 'recall', 'f1_score',\n        'box_loss', 'cls_loss', 'dfl_loss', 'val_box_loss', 'val_cls_loss', 'val_dfl_loss',\n        'total_parameters', 'trainable_parameters', 'training_time_seconds',\n        'batch_size', 'image_size', 'epochs', 'learning_rate', 'weight_decay',\n        'box_weight', 'cls_weight', 'dfl_weight', 'inference_time_ms', 'fps',\n        'duration_seconds', 'gflops'\n    ]\n    \n    for col in numeric_columns:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n    \n    # Convert duration to hours\n    if 'duration_seconds' in df.columns:\n        df['duration_hours'] = df['duration_seconds'] / 3600\n        \n    if 'training_time_seconds' in df.columns:\n        df['training_time_hours'] = df['training_time_seconds'] / 3600\n    \n    # Create model variant column\n    df['model_variant'] = df.apply(create_model_variant, axis=1)\n    \n    # Calculate efficiency metrics with proper validation\n    if 'map50' in df.columns and 'training_time_hours' in df.columns:\n        valid_mask = (df['map50'].notna()) & (df['training_time_hours'].notna()) & (df['training_time_hours'] > 0)\n        df.loc[valid_mask, 'training_efficiency'] = df.loc[valid_mask, 'map50'] / df.loc[valid_mask, 'training_time_hours']\n        \n    if 'map50' in df.columns and 'total_parameters' in df.columns:\n        valid_mask = (df['map50'].notna()) & (df['total_parameters'].notna()) & (df['total_parameters'] > 0)\n        df.loc[valid_mask, 'parameter_efficiency'] = df.loc[valid_mask, 'map50'] / (df.loc[valid_mask, 'total_parameters'] / 1e6)  # per million params\n        \n    # Estimate model size in MB (rough approximation)\n    if 'total_parameters' in df.columns:\n        valid_mask = df['total_parameters'].notna() & (df['total_parameters'] > 0)\n        df.loc[valid_mask, 'model_size_mb'] = (df.loc[valid_mask, 'total_parameters'] * 4) / (1024 * 1024)  # Assuming FP32\n    \n    # Add quality score based on data completeness\n    key_metrics = ['map50', 'precision', 'recall', 'f1_score']\n    df['data_quality_score'] = df[key_metrics].notna().sum(axis=1) / len(key_metrics)\n    \n    print(f\"âœ… Data cleaning complete\")\n    print(f\"   ðŸ“Š Rows processed: {len(df)}\")\n    print(f\"   ðŸŽ¯ Average data quality: {df['data_quality_score'].mean():.2f}\")\n    \n    return df\n\n\nprint(\"âœ… Enhanced data fetching functions defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Enhanced Data Fetching with GFLOPs and Media Support",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Fetch data from WandB with enhanced validation\nif WANDB_AUTHENTICATED:\n    df = fetch_wandb_runs(WANDB_PROJECT, WANDB_ENTITY)\n    \n    if len(df) > 0:\n        # Display basic information about the dataset\n        print(f\"\\nðŸ“Š DATASET OVERVIEW:\")\n        print(f\"   Total completed experiments: {len(df)}\")\n        print(f\"   Unique model types: {df['model_type'].nunique()}\")\n        print(f\"   Unique attention mechanisms: {df['attention_mechanism'].nunique()}\")\n        print(f\"   Date range: {df['created_at'].min()} to {df['created_at'].max()}\")\n        \n        # Check data quality\n        key_metrics = ['map50', 'map50_95', 'precision', 'recall', 'f1_score', 'gflops']\n        print(f\"\\nðŸ” FINAL DATA QUALITY CHECK:\")\n        for metric in key_metrics:\n            if metric in df.columns:\n                valid_count = df[metric].notna().sum()\n                percentage = (valid_count / len(df)) * 100\n                status = \"âœ…\" if percentage >= 80 else \"âš ï¸ \" if percentage >= 50 else \"âŒ\"\n                print(f\"   {status} {metric}: {valid_count}/{len(df)} ({percentage:.1f}%)\")\n        \n        # Display sample data with enhanced columns\n        print(f\"\\nðŸ“‹ SAMPLE DATA (Enhanced with GFLOPs):\")\n        display_columns = ['run_name', 'model_variant', 'map50', 'map50_95', 'f1_score', 'gflops', 'fps', 'training_time_hours']\n        available_columns = [col for col in display_columns if col in df.columns]\n        sample_df = df[available_columns].head()\n        \n        # Round numeric columns for display\n        numeric_cols = sample_df.select_dtypes(include=[np.number]).columns\n        sample_df[numeric_cols] = sample_df[numeric_cols].round(4)\n        \n        print(sample_df.to_string(index=False))\n        \n        # Show data readiness status\n        if df['data_quality_score'].mean() >= 0.8:\n            print(f\"\\nâœ… DATA READY FOR ANALYSIS - High quality data available\")\n        elif df['data_quality_score'].mean() >= 0.6:\n            print(f\"\\nâš ï¸  DATA PARTIALLY READY - Some metrics may be limited\")\n        else:\n            print(f\"\\nâŒ DATA QUALITY CONCERNS - Analysis may be limited\")\n            \n    else:\n        print(f\"\\nâŒ NO DATA AVAILABLE FOR ANALYSIS\")\n        print(\"Please check your WandB project and ensure experiments have completed successfully\")\nelse:\n    print(f\"\\nâŒ WANDB AUTHENTICATION REQUIRED\")\n    print(\"Please run the authentication cell above first\")\n    df = pd.DataFrame()  # Empty dataframe as fallback",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. WandB Authentication & API Validation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wandb_runs(project_name, entity=None):\n",
    "    \"\"\"\n",
    "    Fetch all runs from WandB project and extract comprehensive metrics.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Comprehensive experiment results\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”„ Fetching runs from WandB...\")\n",
    "    \n",
    "    # Initialize WandB API\n",
    "    api = wandb.Api()\n",
    "    \n",
    "    # Get project path\n",
    "    project_path = f\"{entity}/{project_name}\" if entity else project_name\n",
    "    runs = api.runs(project_path)\n",
    "    \n",
    "    print(f\"ðŸ“¥ Found {len(runs)} runs\")\n",
    "    \n",
    "    # Extract run data\n",
    "    runs_data = []\n",
    "    \n",
    "    for i, run in enumerate(runs):\n",
    "        if i % 5 == 0:  # Progress indicator\n",
    "            print(f\"   Processing run {i+1}/{len(runs)}...\")\n",
    "            \n",
    "        try:\n",
    "            # Basic run information\n",
    "            run_data = {\n",
    "                'run_id': run.id,\n",
    "                'run_name': run.name,\n",
    "                'state': run.state,\n",
    "                'created_at': run.created_at,\n",
    "                'duration_seconds': run._attrs.get('runtime', 0),\n",
    "                'tags': run.tags,\n",
    "                'notes': run.notes,\n",
    "                'url': run.url\n",
    "            }\n",
    "            \n",
    "            # Extract configuration\n",
    "            config = run.config\n",
    "            \n",
    "            # Model configuration\n",
    "            model_config = config.get('model_config', {})\n",
    "            run_data.update({\n",
    "                'model_type': model_config.get('type', 'unknown'),\n",
    "                'attention_mechanism': model_config.get('attention_mechanism', 'none'),\n",
    "                'model_config_path': model_config.get('config_path', ''),\n",
    "                'pretrained': model_config.get('pretrained', True)\n",
    "            })\n",
    "            \n",
    "            # Training configuration  \n",
    "            training_config = config.get('training_config', {})\n",
    "            run_data.update({\n",
    "                'batch_size': training_config.get('batch', 'unknown'),\n",
    "                'image_size': training_config.get('imgsz', 640),\n",
    "                'epochs': training_config.get('epochs', 'unknown'),\n",
    "                'optimizer': training_config.get('optimizer', 'unknown'),\n",
    "                'learning_rate': training_config.get('lr0', 'unknown'),\n",
    "                'weight_decay': training_config.get('weight_decay', 'unknown'),\n",
    "                'amp': training_config.get('amp', True),\n",
    "                'cache': training_config.get('cache', True)\n",
    "            })\n",
    "            \n",
    "            # Loss configuration\n",
    "            loss_config = training_config.get('loss', {})\n",
    "            run_data.update({\n",
    "                'loss_type': loss_config.get('type', 'standard'),\n",
    "                'box_weight': loss_config.get('box_weight', 7.5),\n",
    "                'cls_weight': loss_config.get('cls_weight', 0.5),\n",
    "                'dfl_weight': loss_config.get('dfl_weight', 1.5)\n",
    "            })\n",
    "            \n",
    "            # Data configuration\n",
    "            data_config = config.get('data_config', {})\n",
    "            run_data['num_classes'] = data_config.get('num_classes', 6)\n",
    "            \n",
    "            # Extract final metrics from summary\n",
    "            summary = run.summary\n",
    "            \n",
    "            # Performance metrics with multiple possible keys\n",
    "            metrics_mapping = {\n",
    "                'map50': ['metrics/mAP50', 'metrics/mAP50(B)', 'val/mAP50', 'mAP50'],\n",
    "                'map50_95': ['metrics/mAP50-95', 'metrics/mAP50-95(B)', 'val/mAP50-95', 'mAP50-95'],\n",
    "                'precision': ['metrics/precision(B)', 'metrics/precision', 'val/precision', 'precision'],\n",
    "                'recall': ['metrics/recall(B)', 'metrics/recall', 'val/recall', 'recall'],\n",
    "                'f1_score': ['metrics/F1(B)', 'metrics/F1', 'val/F1', 'f1'],\n",
    "                'box_loss': ['train/box_loss', 'box_loss'],\n",
    "                'cls_loss': ['train/cls_loss', 'cls_loss'], \n",
    "                'dfl_loss': ['train/dfl_loss', 'dfl_loss'],\n",
    "                'val_box_loss': ['val/box_loss'],\n",
    "                'val_cls_loss': ['val/cls_loss'],\n",
    "                'val_dfl_loss': ['val/dfl_loss'],\n",
    "                'total_parameters': ['model/total_parameters'],\n",
    "                'trainable_parameters': ['model/trainable_parameters'],\n",
    "                'training_time_seconds': ['final/training_time_seconds', 'training_time']\n",
    "            }\n",
    "            \n",
    "            # Extract metrics with fallback\n",
    "            for metric_name, possible_keys in metrics_mapping.items():\n",
    "                value = None\n",
    "                for key in possible_keys:\n",
    "                    if key in summary:\n",
    "                        value = summary[key]\n",
    "                        break\n",
    "                run_data[metric_name] = value\n",
    "                \n",
    "            # Speed metrics (if available)\n",
    "            if 'speed/inference' in summary:\n",
    "                run_data['inference_time_ms'] = summary['speed/inference']\n",
    "                run_data['fps'] = 1000.0 / summary['speed/inference'] if summary['speed/inference'] > 0 else None\n",
    "            elif 'speed' in summary and isinstance(summary['speed'], dict):\n",
    "                speed_dict = summary['speed']\n",
    "                if 'inference' in speed_dict:\n",
    "                    run_data['inference_time_ms'] = speed_dict['inference']\n",
    "                    run_data['fps'] = 1000.0 / speed_dict['inference'] if speed_dict['inference'] > 0 else None\n",
    "                    \n",
    "            # Per-class metrics (HRIPCB has 6 classes)\n",
    "            class_names = ['Missing_hole', 'Mouse_bite', 'Open_circuit', 'Short', 'Spurious_copper', 'Spur']\n",
    "            for i, class_name in enumerate(class_names):\n",
    "                class_map_key = f'metrics/mAP50(B)_{i}'\n",
    "                if class_map_key in summary:\n",
    "                    run_data[f'{class_name}_mAP50'] = summary[class_map_key]\n",
    "                    \n",
    "            # Extract phase and experiment type from tags\n",
    "            phase = 'Unknown'\n",
    "            experiment_type = 'Unknown'\n",
    "            \n",
    "            for tag in run.tags:\n",
    "                if 'phase_1' in tag:\n",
    "                    phase = 'Phase 1: Baselines'\n",
    "                elif 'phase_2' in tag:\n",
    "                    phase = 'Phase 2: Attention'\n",
    "                elif 'phase_3' in tag:\n",
    "                    phase = 'Phase 3: Loss/Resolution'\n",
    "                    \n",
    "                if 'model_scaling' in tag:\n",
    "                    experiment_type = 'Model Scaling'\n",
    "                elif 'attention_study' in tag:\n",
    "                    experiment_type = 'Attention Study'\n",
    "                elif 'architecture_study' in tag:\n",
    "                    experiment_type = 'Architecture Study'\n",
    "                elif 'resolution_study' in tag:\n",
    "                    experiment_type = 'Resolution Study'\n",
    "                    \n",
    "            run_data['phase'] = phase\n",
    "            run_data['experiment_type'] = experiment_type\n",
    "            \n",
    "            runs_data.append(run_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Error processing run {run.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(runs_data)\n",
    "    \n",
    "    # Data cleaning and preprocessing\n",
    "    df = clean_dataframe(df)\n",
    "    \n",
    "    print(f\"âœ… Successfully processed {len(df)} runs\")\n",
    "    print(f\"   ðŸ“Š Completed: {len(df[df['state'] == 'finished'])}\")\n",
    "    print(f\"   ðŸ”„ Running: {len(df[df['state'] == 'running'])}\")\n",
    "    print(f\"   âŒ Failed: {len(df[df['state'] == 'failed'])}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the dataframe.\n",
    "    \"\"\"\n",
    "    # Convert numeric columns\n",
    "    numeric_columns = [\n",
    "        'map50', 'map50_95', 'precision', 'recall', 'f1_score',\n",
    "        'box_loss', 'cls_loss', 'dfl_loss', 'val_box_loss', 'val_cls_loss', 'val_dfl_loss',\n",
    "        'total_parameters', 'trainable_parameters', 'training_time_seconds',\n",
    "        'batch_size', 'image_size', 'epochs', 'learning_rate', 'weight_decay',\n",
    "        'box_weight', 'cls_weight', 'dfl_weight', 'inference_time_ms', 'fps',\n",
    "        'duration_seconds'\n",
    "    ]\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Convert duration to hours\n",
    "    if 'duration_seconds' in df.columns:\n",
    "        df['duration_hours'] = df['duration_seconds'] / 3600\n",
    "        \n",
    "    if 'training_time_seconds' in df.columns:\n",
    "        df['training_time_hours'] = df['training_time_seconds'] / 3600\n",
    "    \n",
    "    # Create model variant column\n",
    "    df['model_variant'] = df.apply(create_model_variant, axis=1)\n",
    "    \n",
    "    # Calculate efficiency metrics\n",
    "    if 'map50' in df.columns and 'training_time_hours' in df.columns:\n",
    "        df['training_efficiency'] = df['map50'] / df['training_time_hours']\n",
    "        \n",
    "    if 'map50' in df.columns and 'total_parameters' in df.columns:\n",
    "        df['parameter_efficiency'] = df['map50'] / (df['total_parameters'] / 1e6)  # per million params\n",
    "        \n",
    "    # Estimate model size in MB (rough approximation)\n",
    "    if 'total_parameters' in df.columns:\n",
    "        df['model_size_mb'] = (df['total_parameters'] * 4) / (1024 * 1024)  # Assuming FP32\n",
    "    \n",
    "    # Filter only completed runs for analysis\n",
    "    df_completed = df[df['state'] == 'finished'].copy()\n",
    "    \n",
    "    return df_completed\n",
    "\n",
    "\n",
    "def create_model_variant(row):\n",
    "    \"\"\"\n",
    "    Create a descriptive model variant name.\n",
    "    \"\"\"\n",
    "    model_type = row.get('model_type', 'unknown')\n",
    "    attention = row.get('attention_mechanism', 'none')\n",
    "    image_size = row.get('image_size', 640)\n",
    "    \n",
    "    # Clean up model type\n",
    "    model_str = model_type.upper() if pd.notna(model_type) else 'UNKNOWN'\n",
    "    \n",
    "    # Clean up attention mechanism\n",
    "    if pd.isna(attention) or attention == 'none' or attention == 'unknown':\n",
    "        attention_str = \"\"\n",
    "    else:\n",
    "        attention_str = f\" + {attention.upper()}\"\n",
    "    \n",
    "    # Add resolution if different from standard\n",
    "    if pd.notna(image_size) and image_size != 640:\n",
    "        size_str = f\" ({int(image_size)}px)\"\n",
    "    else:\n",
    "        size_str = \"\"\n",
    "    \n",
    "    return f\"{model_str}{attention_str}{size_str}\"\n",
    "\n",
    "\n",
    "print(\"âœ… Data fetching functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fetch Experimental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate summary statistics\nif len(df) > 0:\n    generate_summary_statistics(df)\n\n# Create comprehensive comparison table with GFLOPs\nprint(\"\\n\" + \"=\"*80)\nprint(\"ðŸ“‹ COMPREHENSIVE COMPARISON TABLE (Enhanced with GFLOPs)\")\nprint(\"=\"*80)\n\nif len(df) > 0:\n    # Select key columns for comparison including GFLOPs\n    comparison_columns = [\n        'model_variant', \n        # Accuracy metrics\n        'map50', 'map50_95', 'precision', 'recall', 'f1_score',\n        # Efficiency metrics with GFLOPs\n        'inference_time_ms', 'fps', 'gflops', 'total_parameters', 'model_size_mb',\n        # Training metrics\n        'training_time_hours'\n    ]\n    \n    # Filter available columns\n    available_comparison_columns = [col for col in comparison_columns if col in df.columns]\n    \n    # Create comparison table\n    comparison_df = df[available_comparison_columns].copy()\n    \n    # Round numeric columns for display\n    numeric_columns = comparison_df.select_dtypes(include=[np.number]).columns\n    comparison_df[numeric_columns] = comparison_df[numeric_columns].round(4)\n    \n    # Sort by mAP@0.5 if available\n    if 'map50' in comparison_df.columns:\n        comparison_df = comparison_df.sort_values('map50', ascending=False)\n    \n    print(\"ðŸ“Š ACCURACY & EFFICIENCY METRICS:\")\n    print(\"â€¢ Accuracy: mAP@0.5, mAP@0.5:0.95, Precision, Recall, F1\")\n    print(\"â€¢ Efficiency: Inference Time (ms), FPS, GFLOPs, Parameters, Model Size (MB)\")\n    print(\"-\" * 80)\n    print(comparison_df.to_string(index=False))\n    \n    # Create separate detailed efficiency table\n    if all(col in df.columns for col in ['gflops', 'fps', 'model_size_mb']):\n        print(f\"\\nðŸ“Š DETAILED EFFICIENCY ANALYSIS:\")\n        efficiency_df = df[['model_variant', 'map50', 'fps', 'gflops', 'model_size_mb', 'parameter_efficiency']].copy()\n        efficiency_df = efficiency_df.dropna()\n        \n        if len(efficiency_df) > 0:\n            # Add efficiency ratio (mAP per GFLOPs)\n            efficiency_df['map_per_gflop'] = efficiency_df['map50'] / efficiency_df['gflops']\n            efficiency_df = efficiency_df.round(4)\n            efficiency_df = efficiency_df.sort_values('map_per_gflop', ascending=False)\n            \n            print(\"â€¢ Efficiency Ratio: mAP@0.5 per GFLOPs (higher is better)\")\n            print(\"-\" * 60)\n            print(efficiency_df[['model_variant', 'map50', 'gflops', 'map_per_gflop', 'fps', 'model_size_mb']].to_string(index=False))\n    \n    # Save to CSV\n    if SAVE_PLOTS:\n        csv_path = OUTPUT_DIR / \"enhanced_experiment_comparison_table.csv\"\n        comparison_df.to_csv(csv_path, index=False)\n        print(f\"\\nðŸ’¾ Enhanced comparison table saved to: {csv_path}\")\n        \n        # Also save efficiency analysis\n        if 'efficiency_df' in locals() and len(efficiency_df) > 0:\n            eff_csv_path = OUTPUT_DIR / \"efficiency_analysis_table.csv\"\n            efficiency_df.to_csv(eff_csv_path, index=False)\n            print(f\"ðŸ’¾ Efficiency analysis saved to: {eff_csv_path}\")\nelse:\n    print(\"âš ï¸  No data available for comparison\")\n    \n# Add media retrieval demonstration if data is available\nif len(df) > 0 and WANDB_AUTHENTICATED:\n    print(f\"\\n\" + \"=\"*80)\n    print(\"ðŸŽ¨ ULTRALYTICS MEDIA RETRIEVAL DEMONSTRATION\")\n    print(\"=\"*80)\n    \n    try:\n        # Demonstrate media retrieval for top 3 runs\n        demonstrate_media_retrieval(df, n_runs=3)\n    except Exception as e:\n        print(f\"âš ï¸  Media retrieval demonstration failed: {e}\")\n        print(\"This is normal if your experiments don't have logged media artifacts\")\nelse:\n    print(f\"\\nâš ï¸  Skipping media retrieval demonstration (no data or not authenticated)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_statistics(df):\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary statistics.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“Š PERFORMANCE SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Overall statistics\n",
    "    metrics = ['map50', 'map50_95', 'precision', 'recall', 'f1_score']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric in df.columns and not df[metric].isna().all():\n",
    "            values = df[metric].dropna()\n",
    "            print(f\"\\n{metric.upper()}:\")\n",
    "            print(f\"   Mean: {values.mean():.4f} Â± {values.std():.4f}\")\n",
    "            print(f\"   Range: [{values.min():.4f}, {values.max():.4f}]\")\n",
    "            print(f\"   Best model: {df.loc[values.idxmax(), 'model_variant']}\")\n",
    "    \n",
    "    # Model comparison\n",
    "    if 'model_type' in df.columns and 'map50' in df.columns:\n",
    "        print(f\"\\nðŸ“ˆ MODEL TYPE COMPARISON (mAP@0.5):\")\n",
    "        model_stats = df.groupby('model_type')['map50'].agg(['count', 'mean', 'std']).round(4)\n",
    "        print(model_stats)\n",
    "    \n",
    "    # Attention mechanism comparison\n",
    "    if 'attention_mechanism' in df.columns and 'map50' in df.columns:\n",
    "        print(f\"\\nðŸŽ¯ ATTENTION MECHANISM COMPARISON (mAP@0.5):\")\n",
    "        attention_stats = df.groupby('attention_mechanism')['map50'].agg(['count', 'mean', 'std']).round(4)\n",
    "        print(attention_stats)\n",
    "    \n",
    "    # Training efficiency\n",
    "    if 'training_time_hours' in df.columns:\n",
    "        total_training_time = df['training_time_hours'].sum()\n",
    "        print(f\"\\nâ±ï¸  TRAINING EFFICIENCY:\")\n",
    "        print(f\"   Total training time: {total_training_time:.1f} hours\")\n",
    "        print(f\"   Average per experiment: {df['training_time_hours'].mean():.1f} hours\")\n",
    "        \n",
    "        if 'training_efficiency' in df.columns:\n",
    "            best_efficiency_idx = df['training_efficiency'].idxmax()\n",
    "            best_efficient_model = df.loc[best_efficiency_idx, 'model_variant']\n",
    "            best_efficiency_score = df.loc[best_efficiency_idx, 'training_efficiency']\n",
    "            print(f\"   Most efficient: {best_efficient_model} ({best_efficiency_score:.4f} mAP/hour)\")\n",
    "\n",
    "\n",
    "def identify_pareto_optimal(df, x_metric, y_metric, maximize_x=True, maximize_y=True):\n",
    "    \"\"\"\n",
    "    Identify Pareto optimal points for trade-off analysis.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with experimental results\n",
    "        x_metric: Column name for x-axis metric\n",
    "        y_metric: Column name for y-axis metric  \n",
    "        maximize_x: Whether to maximize x_metric (True) or minimize (False)\n",
    "        maximize_y: Whether to maximize y_metric (True) or minimize (False)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Pareto optimal points\n",
    "    \"\"\"\n",
    "    # Filter valid data\n",
    "    valid_data = df.dropna(subset=[x_metric, y_metric]).copy()\n",
    "    \n",
    "    if len(valid_data) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Adjust for minimization (convert to maximization problem)\n",
    "    x_values = valid_data[x_metric].values * (1 if maximize_x else -1)\n",
    "    y_values = valid_data[y_metric].values * (1 if maximize_y else -1)\n",
    "    \n",
    "    # Find Pareto optimal points\n",
    "    pareto_mask = np.zeros(len(valid_data), dtype=bool)\n",
    "    \n",
    "    for i in range(len(valid_data)):\n",
    "        is_dominated = False\n",
    "        for j in range(len(valid_data)):\n",
    "            if i != j:\n",
    "                # Check if point i is dominated by point j\n",
    "                if (x_values[j] >= x_values[i] and y_values[j] >= y_values[i] and \n",
    "                    (x_values[j] > x_values[i] or y_values[j] > y_values[i])):\n",
    "                    is_dominated = True\n",
    "                    break\n",
    "        \n",
    "        if not is_dominated:\n",
    "            pareto_mask[i] = True\n",
    "    \n",
    "    pareto_optimal = valid_data[pareto_mask].copy()\n",
    "    \n",
    "    # Sort by x_metric for plotting\n",
    "    pareto_optimal = pareto_optimal.sort_values(x_metric, ascending=not maximize_x)\n",
    "    \n",
    "    return pareto_optimal\n",
    "\n",
    "\n",
    "def filter_edge_suitable_models(df):\n",
    "    \"\"\"\n",
    "    Filter models suitable for edge deployment based on constraints.\n",
    "    \"\"\"\n",
    "    edge_suitable = df.copy()\n",
    "    \n",
    "    # Apply constraints\n",
    "    constraints_applied = []\n",
    "    \n",
    "    if 'model_size_mb' in df.columns:\n",
    "        before_count = len(edge_suitable)\n",
    "        edge_suitable = edge_suitable[edge_suitable['model_size_mb'] <= EDGE_CONSTRAINTS['max_model_size_mb']]\n",
    "        constraints_applied.append(f\"Model size â‰¤ {EDGE_CONSTRAINTS['max_model_size_mb']}MB: {before_count} â†’ {len(edge_suitable)}\")\n",
    "    \n",
    "    if 'fps' in df.columns:\n",
    "        before_count = len(edge_suitable)\n",
    "        edge_suitable = edge_suitable[edge_suitable['fps'] >= EDGE_CONSTRAINTS['min_fps']]\n",
    "        constraints_applied.append(f\"FPS â‰¥ {EDGE_CONSTRAINTS['min_fps']}: {before_count} â†’ {len(edge_suitable)}\")\n",
    "    \n",
    "    if 'map50' in df.columns:\n",
    "        before_count = len(edge_suitable)\n",
    "        edge_suitable = edge_suitable[edge_suitable['map50'] >= EDGE_CONSTRAINTS['min_map50']]\n",
    "        constraints_applied.append(f\"mAP@0.5 â‰¥ {EDGE_CONSTRAINTS['min_map50']}: {before_count} â†’ {len(edge_suitable)}\")\n",
    "    \n",
    "    if 'inference_time_ms' in df.columns:\n",
    "        before_count = len(edge_suitable)\n",
    "        edge_suitable = edge_suitable[edge_suitable['inference_time_ms'] <= EDGE_CONSTRAINTS['max_inference_time_ms']]\n",
    "        constraints_applied.append(f\"Inference time â‰¤ {EDGE_CONSTRAINTS['max_inference_time_ms']}ms: {before_count} â†’ {len(edge_suitable)}\")\n",
    "    \n",
    "    print(f\"ðŸ” Edge Deployment Filtering:\")\n",
    "    print(f\"   Original models: {len(df)}\")\n",
    "    for constraint in constraints_applied:\n",
    "        print(f\"   {constraint}\")\n",
    "    print(f\"   âœ… Edge-suitable models: {len(edge_suitable)}\")\n",
    "    \n",
    "    return edge_suitable\n",
    "\n",
    "\n",
    "print(\"âœ… Analysis functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "generate_summary_statistics(df)\n",
    "\n",
    "# Create comprehensive comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ COMPREHENSIVE COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(df) > 0:\n",
    "    # Select key columns for comparison\n",
    "    comparison_columns = [\n",
    "        'model_variant', 'map50', 'map50_95', 'precision', 'recall', 'f1_score',\n",
    "        'training_time_hours', 'total_parameters', 'model_size_mb',\n",
    "        'fps', 'inference_time_ms'\n",
    "    ]\n",
    "    \n",
    "    # Filter available columns\n",
    "    available_comparison_columns = [col for col in comparison_columns if col in df.columns]\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_df = df[available_comparison_columns].copy()\n",
    "    \n",
    "    # Round numeric columns for display\n",
    "    numeric_columns = comparison_df.select_dtypes(include=[np.number]).columns\n",
    "    comparison_df[numeric_columns] = comparison_df[numeric_columns].round(4)\n",
    "    \n",
    "    # Sort by mAP@0.5 if available\n",
    "    if 'map50' in comparison_df.columns:\n",
    "        comparison_df = comparison_df.sort_values('map50', ascending=False)\n",
    "    \n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    if SAVE_PLOTS:\n",
    "        csv_path = OUTPUT_DIR / \"experiment_comparison_table.csv\"\n",
    "        comparison_df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\nðŸ’¾ Comparison table saved to: {csv_path}\")\nelse:\n    print(\"âš ï¸  No data available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_comparison_plots(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive performance comparison visualizations.\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        print(\"âš ï¸  No data available for plotting\")\n",
    "        return\n",
    "    \n",
    "    # Set up the plotting grid\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('PCB Defect Detection: Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Model Type Comparison (Box Plot)\n",
    "    if 'model_type' in df.columns and 'map50' in df.columns:\n",
    "        valid_data = df.dropna(subset=['model_type', 'map50'])\n",
    "        if len(valid_data) > 0:\n",
    "            sns.boxplot(data=valid_data, x='model_type', y='map50', ax=axes[0,0])\n",
    "            axes[0,0].set_title('Model Type Performance Comparison')\n",
    "            axes[0,0].set_xlabel('Model Type')\n",
    "            axes[0,0].set_ylabel('mAP@0.5')\n",
    "            axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Attention Mechanism Comparison\n",
    "    if 'attention_mechanism' in df.columns and 'map50' in df.columns:\n",
    "        valid_data = df.dropna(subset=['attention_mechanism', 'map50'])\n",
    "        if len(valid_data) > 0:\n",
    "            sns.boxplot(data=valid_data, x='attention_mechanism', y='map50', ax=axes[0,1])\n",
    "            axes[0,1].set_title('Attention Mechanism Comparison')\n",
    "            axes[0,1].set_xlabel('Attention Mechanism')\n",
    "            axes[0,1].set_ylabel('mAP@0.5')\n",
    "            axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Resolution Study\n",
    "    if 'image_size' in df.columns and 'map50' in df.columns:\n",
    "        valid_data = df.dropna(subset=['image_size', 'map50'])\n",
    "        if len(valid_data) > 0:\n",
    "            sns.scatterplot(data=valid_data, x='image_size', y='map50', \n",
    "                          hue='model_type', size='total_parameters', \n",
    "                          sizes=(50, 200), alpha=0.7, ax=axes[1,0])\n",
    "            axes[1,0].set_title('Resolution vs Performance')\n",
    "            axes[1,0].set_xlabel('Image Size (pixels)')\n",
    "            axes[1,0].set_ylabel('mAP@0.5')\n",
    "    \n",
    "    # 4. Training Efficiency\n",
    "    if 'training_time_hours' in df.columns and 'map50' in df.columns:\n",
    "        valid_data = df.dropna(subset=['training_time_hours', 'map50'])\n",
    "        if len(valid_data) > 0:\n",
    "            sns.scatterplot(data=valid_data, x='training_time_hours', y='map50',\n",
    "                          hue='model_type', size='total_parameters',\n",
    "                          sizes=(50, 200), alpha=0.7, ax=axes[1,1])\n",
    "            axes[1,1].set_title('Training Time vs Performance')\n",
    "            axes[1,1].set_xlabel('Training Time (hours)')\n",
    "            axes[1,1].set_ylabel('mAP@0.5')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if SAVE_PLOTS:\n",
    "        plt.savefig(OUTPUT_DIR / 'performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"ðŸ’¾ Performance comparison plot saved\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_metrics_correlation_heatmap(df):\n",
    "    \"\"\"\n",
    "    Create correlation heatmap between different metrics.\n",
    "    \"\"\"\n",
    "    metrics_columns = ['map50', 'map50_95', 'precision', 'recall', 'f1_score',\n",
    "                      'training_time_hours', 'total_parameters']\n",
    "    \n",
    "    available_metrics = [col for col in metrics_columns if col in df.columns and not df[col].isna().all()]\n",
    "    \n",
    "    if len(available_metrics) < 2:\n",
    "        print(\"âš ï¸  Insufficient metrics for correlation analysis\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df[available_metrics].corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.3f', cbar_kws={'label': 'Correlation Coefficient'})\n",
    "    \n",
    "    plt.title('Metrics Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if SAVE_PLOTS:\n",
    "        plt.savefig(OUTPUT_DIR / 'metrics_correlation.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"ðŸ’¾ Correlation heatmap saved\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create visualizations\n",
    "print(\"ðŸ“Š Creating performance visualizations...\")\n",
    "create_performance_comparison_plots(df)\n",
    "create_metrics_correlation_heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pareto Analysis for Edge Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pareto_analysis(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive Pareto analysis for edge deployment optimization.\n",
    "    \"\"\"\n",
    "    print(\"ðŸŽ¯ PARETO ANALYSIS FOR EDGE DEPLOYMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"âš ï¸  No data available for Pareto analysis\")\n",
    "        return\n",
    "    \n",
    "    # Create subplot figure\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. mAP vs FPS (Performance vs Speed)\n",
    "    if 'fps' in df.columns and 'map50' in df.columns:\n",
    "        ax1 = plt.subplot(2, 3, 1)\n",
    "        \n",
    "        valid_data = df.dropna(subset=['fps', 'map50'])\n",
    "        if len(valid_data) > 0:\n",
    "            # Find Pareto optimal points\n",
    "            pareto_points = identify_pareto_optimal(valid_data, 'fps', 'map50', \n",
    "                                                  maximize_x=True, maximize_y=True)\n",
    "            \n",
    "            # Plot all points\n",
    "            scatter = plt.scatter(valid_data['fps'], valid_data['map50'], \n",
    "                                c='lightblue', alpha=0.6, s=50, label='All Models')\n",
    "            \n",
    "            # Plot Pareto optimal points\n",
    "            if len(pareto_points) > 0:\n",
    "                plt.scatter(pareto_points['fps'], pareto_points['map50'], \n",
    "                          c='red', s=100, label='Pareto Optimal', marker='*')\n",
    "                \n",
    "                # Connect Pareto points\n",
    "                pareto_sorted = pareto_points.sort_values('fps')\n",
    "                plt.plot(pareto_sorted['fps'], pareto_sorted['map50'], \n",
    "                        'r--', alpha=0.7, label='Pareto Front')\n",
    "                \n",
    "                # Annotate Pareto points\n",
    "                for _, row in pareto_points.iterrows():\n",
    "                    plt.annotate(row['model_variant'], \n",
    "                               (row['fps'], row['map50']),\n",
    "                               xytext=(5, 5), textcoords='offset points', \n",
    "                               fontsize=8, ha='left')\n",
    "            \n",
    "            plt.xlabel('FPS (Inference Speed)')\n",
    "            plt.ylabel('mAP@0.5 (Performance)')\n",
    "            plt.title('Performance vs Speed Trade-off')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. mAP vs Model Size (Performance vs Efficiency)\n",
    "    if 'model_size_mb' in df.columns and 'map50' in df.columns:\n",
    "        ax2 = plt.subplot(2, 3, 2)\n",
    "        \n",
    "        valid_data = df.dropna(subset=['model_size_mb', 'map50'])\n",
    "        if len(valid_data) > 0:\n",
    "            # Find Pareto optimal points (minimize size, maximize performance)\n",
    "            pareto_points = identify_pareto_optimal(valid_data, 'model_size_mb', 'map50',\n",
    "                                                  maximize_x=False, maximize_y=True)\n",
    "            \n",
    "            # Plot all points\n",
    "            plt.scatter(valid_data['model_size_mb'], valid_data['map50'],\n",
    "                       c='lightgreen', alpha=0.6, s=50, label='All Models')\n",
    "            \n",
    "            # Plot Pareto optimal points\n",
    "            if len(pareto_points) > 0:\n",
    "                plt.scatter(pareto_points['model_size_mb'], pareto_points['map50'],\n",
    "                          c='red', s=100, label='Pareto Optimal', marker='*')\n",
    "                \n",
    "                # Connect Pareto points\n",
    "                pareto_sorted = pareto_points.sort_values('model_size_mb')\n",
    "                plt.plot(pareto_sorted['model_size_mb'], pareto_sorted['map50'],\n",
    "                        'r--', alpha=0.7, label='Pareto Front')\n",
    "                \n",
    "                # Annotate Pareto points\n",
    "                for _, row in pareto_points.iterrows():\n",
    "                    plt.annotate(row['model_variant'],\n",
    "                               (row['model_size_mb'], row['map50']),\n",
    "                               xytext=(5, 5), textcoords='offset points',\n",
    "                               fontsize=8, ha='left')\n",
    "            \n",
    "            plt.xlabel('Model Size (MB)')\n",
    "            plt.ylabel('mAP@0.5 (Performance)')\n",
    "            plt.title('Performance vs Model Size Trade-off')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Training Time vs Performance\n",
    "    if 'training_time_hours' in df.columns and 'map50' in df.columns:\n",
    "        ax3 = plt.subplot(2, 3, 3)\n",
    "        \n",
    "        valid_data = df.dropna(subset=['training_time_hours', 'map50'])\n",
    "        if len(valid_data) > 0:\n",
    "            # Color by model type\n",
    "            if 'model_type' in df.columns:\n",
    "                for model_type in valid_data['model_type'].unique():\n",
    "                    model_data = valid_data[valid_data['model_type'] == model_type]\n",
    "                    plt.scatter(model_data['training_time_hours'], model_data['map50'],\n",
    "                              label=model_type, alpha=0.7, s=60)\n",
    "            else:\n",
    "                plt.scatter(valid_data['training_time_hours'], valid_data['map50'],\n",
    "                          alpha=0.7, s=60)\n",
    "            \n",
    "            plt.xlabel('Training Time (hours)')\n",
    "            plt.ylabel('mAP@0.5 (Performance)')\n",
    "            plt.title('Training Efficiency Analysis')\n",
    "            if 'model_type' in df.columns:\n",
    "                plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. 3D Performance-Speed-Size Plot\n",
    "    if all(col in df.columns for col in ['fps', 'map50', 'model_size_mb']):\n",
    "        ax4 = plt.subplot(2, 3, 4, projection='3d')\n",
    "        \n",
    "        valid_data = df.dropna(subset=['fps', 'map50', 'model_size_mb'])\n",
    "        if len(valid_data) > 0:\n",
    "            scatter = ax4.scatter(valid_data['fps'], valid_data['model_size_mb'], valid_data['map50'],\n",
    "                                c=valid_data['map50'], cmap='viridis', s=60, alpha=0.7)\n",
    "            \n",
    "            ax4.set_xlabel('FPS')\n",
    "            ax4.set_ylabel('Model Size (MB)')\n",
    "            ax4.set_zlabel('mAP@0.5')\n",
    "            ax4.set_title('3D Performance Trade-off')\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(scatter, ax=ax4, shrink=0.5, label='mAP@0.5')\n",
    "    \n",
    "    # 5. Edge Deployment Constraints Visualization\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    \n",
    "    # Filter edge-suitable models\n",
    "    edge_suitable = filter_edge_suitable_models(df)\n",
    "    \n",
    "    if 'fps' in df.columns and 'map50' in df.columns:\n",
    "        valid_all = df.dropna(subset=['fps', 'map50'])\n",
    "        valid_edge = edge_suitable.dropna(subset=['fps', 'map50']) if len(edge_suitable) > 0 else pd.DataFrame()\n",
    "        \n",
    "        if len(valid_all) > 0:\n",
    "            # Plot all models\n",
    "            plt.scatter(valid_all['fps'], valid_all['map50'], \n",
    "                       c='lightgray', alpha=0.5, s=50, label='All Models')\n",
    "            \n",
    "            # Plot edge-suitable models\n",
    "            if len(valid_edge) > 0:\n",
    "                plt.scatter(valid_edge['fps'], valid_edge['map50'],\n",
    "                          c='green', s=80, alpha=0.8, label='Edge Suitable')\n",
    "                \n",
    "                # Annotate edge-suitable models\n",
    "                for _, row in valid_edge.iterrows():\n",
    "                    plt.annotate(row['model_variant'],\n",
    "                               (row['fps'], row['map50']),\n",
    "                               xytext=(5, 5), textcoords='offset points',\n",
    "                               fontsize=8, ha='left')\n",
    "            \n",
    "            # Add constraint lines\n",
    "            plt.axhline(y=EDGE_CONSTRAINTS['min_map50'], color='red', \n",
    "                       linestyle='--', alpha=0.7, label=f\"Min mAP@0.5 ({EDGE_CONSTRAINTS['min_map50']})\")\n",
    "            plt.axvline(x=EDGE_CONSTRAINTS['min_fps'], color='blue', \n",
    "                       linestyle='--', alpha=0.7, label=f\"Min FPS ({EDGE_CONSTRAINTS['min_fps']})\")\n",
    "            \n",
    "            plt.xlabel('FPS (Inference Speed)')\n",
    "            plt.ylabel('mAP@0.5 (Performance)')\n",
    "            plt.title('Edge Deployment Suitability')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Model Efficiency Summary\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    \n",
    "    if 'parameter_efficiency' in df.columns and 'training_efficiency' in df.columns:\n",
    "        valid_data = df.dropna(subset=['parameter_efficiency', 'training_efficiency'])\n",
    "        if len(valid_data) > 0:\n",
    "            scatter = plt.scatter(valid_data['parameter_efficiency'], valid_data['training_efficiency'],\n",
    "                                c=valid_data['map50'] if 'map50' in df.columns else 'blue',\n",
    "                                s=80, alpha=0.7, cmap='viridis')\n",
    "            \n",
    "            # Annotate points\n",
    "            for _, row in valid_data.iterrows():\n",
    "                plt.annotate(row['model_variant'],\n",
    "                           (row['parameter_efficiency'], row['training_efficiency']),\n",
    "                           xytext=(5, 5), textcoords='offset points',\n",
    "                           fontsize=8, ha='left')\n",
    "            \n",
    "            plt.xlabel('Parameter Efficiency (mAP/M params)')\n",
    "            plt.ylabel('Training Efficiency (mAP/hour)')\n",
    "            plt.title('Model Efficiency Comparison')\n",
    "            \n",
    "            if 'map50' in df.columns:\n",
    "                plt.colorbar(scatter, label='mAP@0.5')\n",
    "            \n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if SAVE_PLOTS:\n",
    "        plt.savefig(OUTPUT_DIR / 'pareto_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"ðŸ’¾ Pareto analysis plot saved\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print Pareto optimal models summary\n",
    "    if 'fps' in df.columns and 'map50' in df.columns:\n",
    "        valid_data = df.dropna(subset=['fps', 'map50'])\n",
    "        if len(valid_data) > 0:\n",
    "            pareto_points = identify_pareto_optimal(valid_data, 'fps', 'map50')\n",
    "            \n",
    "            if len(pareto_points) > 0:\n",
    "                print(f\"\\nðŸ† PARETO OPTIMAL MODELS (Performance vs Speed):\")\n",
    "                pareto_display = pareto_points[['model_variant', 'map50', 'fps', 'model_size_mb']].round(4)\n",
    "                print(pareto_display.to_string(index=False))\n",
    "\n",
    "\n",
    "# Run Pareto Analysis\n",
    "create_pareto_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Plotly Visualizations (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOTLY_AVAILABLE and len(df) > 0:\n",
    "    print(\"ðŸŽ¨ Creating interactive Plotly visualizations...\")\n",
    "    \n",
    "    # Interactive 3D Performance Trade-off Plot\n",
    "    if all(col in df.columns for col in ['fps', 'map50', 'model_size_mb', 'model_variant']):\n",
    "        valid_data = df.dropna(subset=['fps', 'map50', 'model_size_mb'])\n",
    "        \n",
    "        if len(valid_data) > 0:\n",
    "            fig = go.Figure(data=[go.Scatter3d(\n",
    "                x=valid_data['fps'],\n",
    "                y=valid_data['model_size_mb'],\n",
    "                z=valid_data['map50'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color=valid_data['map50'],\n",
    "                    colorscale='Viridis',\n",
    "                    showscale=True,\n",
    "                    colorbar=dict(title=\"mAP@0.5\")\n",
    "                ),\n",
    "                text=valid_data['model_variant'],\n",
    "                hovertemplate=\n",
    "                '<b>%{text}</b><br>' +\n",
    "                'FPS: %{x}<br>' +\n",
    "                'Model Size: %{y} MB<br>' +\n",
    "                'mAP@0.5: %{z}<br>' +\n",
    "                '<extra></extra>'\n",
    "            )])\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title='Interactive 3D Performance Trade-off Analysis',\n",
    "                scene=dict(\n",
    "                    xaxis_title='FPS (Inference Speed)',\n",
    "                    yaxis_title='Model Size (MB)',\n",
    "                    zaxis_title='mAP@0.5 (Performance)'\n",
    "                ),\n",
    "                width=800,\n",
    "                height=600\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            if SAVE_PLOTS:\n",
    "                fig.write_html(str(OUTPUT_DIR / 'interactive_3d_analysis.html'))\n",
    "                print(f\"ðŸ’¾ Interactive 3D plot saved as HTML\")\n",
    "    \n",
    "    # Interactive Pareto Plot\n",
    "    if 'fps' in df.columns and 'map50' in df.columns:\n",
    "        valid_data = df.dropna(subset=['fps', 'map50'])\n",
    "        pareto_points = identify_pareto_optimal(valid_data, 'fps', 'map50')\n",
    "        \n",
    "        if len(valid_data) > 0:\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Add all points\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=valid_data['fps'],\n",
    "                y=valid_data['map50'],\n",
    "                mode='markers',\n",
    "                name='All Models',\n",
    "                marker=dict(color='lightblue', size=8, opacity=0.6),\n",
    "                text=valid_data['model_variant'],\n",
    "                hovertemplate='<b>%{text}</b><br>FPS: %{x}<br>mAP@0.5: %{y}<extra></extra>'\n",
    "            ))\n",
    "            \n",
    "            # Add Pareto optimal points\n",
    "            if len(pareto_points) > 0:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=pareto_points['fps'],\n",
    "                    y=pareto_points['map50'],\n",
    "                    mode='markers+lines',\n",
    "                    name='Pareto Optimal',\n",
    "                    marker=dict(color='red', size=12, symbol='star'),\n",
    "                    line=dict(color='red', dash='dash'),\n",
    "                    text=pareto_points['model_variant'],\n",
    "                    hovertemplate='<b>%{text}</b><br>FPS: %{x}<br>mAP@0.5: %{y}<br><b>Pareto Optimal</b><extra></extra>'\n",
    "                ))\n",
    "            \n",
    "            # Add constraint lines\n",
    "            fig.add_hline(y=EDGE_CONSTRAINTS['min_map50'], line_dash=\"dash\", \n",
    "                         line_color=\"red\", annotation_text=f\"Min mAP@0.5 ({EDGE_CONSTRAINTS['min_map50']})\")\n",
    "            fig.add_vline(x=EDGE_CONSTRAINTS['min_fps'], line_dash=\"dash\", \n",
    "                         line_color=\"blue\", annotation_text=f\"Min FPS ({EDGE_CONSTRAINTS['min_fps']})\")\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title='Interactive Performance vs Speed Trade-off',\n",
    "                xaxis_title='FPS (Inference Speed)',\n",
    "                yaxis_title='mAP@0.5 (Performance)',\n",
    "                width=800,\n",
    "                height=600,\n",
    "                showlegend=True\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            if SAVE_PLOTS:\n",
    "                fig.write_html(str(OUTPUT_DIR / 'interactive_pareto_plot.html'))\n",
    "                print(f\"ðŸ’¾ Interactive Pareto plot saved as HTML\")\n",
    "else:\n",
    "    print(\"âš ï¸  Plotly not available or no data for interactive plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_models_for_deployment(df):\n",
    "    \"\"\"\n",
    "    Recommend optimal models for different deployment scenarios.\n",
    "    \"\"\"\n",
    "    print(\"ðŸŽ¯ MODEL RECOMMENDATIONS FOR DEPLOYMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"âš ï¸  No data available for recommendations\")\n",
    "        return\n",
    "    \n",
    "    recommendations = {}\n",
    "    \n",
    "    # 1. Best Overall Performance\n",
    "    if 'map50' in df.columns:\n",
    "        best_performance_idx = df['map50'].idxmax()\n",
    "        best_performance_model = df.loc[best_performance_idx]\n",
    "        \n",
    "        recommendations['best_performance'] = {\n",
    "            'model': best_performance_model['model_variant'],\n",
    "            'map50': best_performance_model['map50'],\n",
    "            'reasoning': 'Highest mAP@0.5 score - best for accuracy-critical applications'\n",
    "        }\n",
    "    \n",
    "    # 2. Best Speed (Highest FPS)\n",
    "    if 'fps' in df.columns:\n",
    "        valid_fps_data = df.dropna(subset=['fps'])\n",
    "        if len(valid_fps_data) > 0:\n",
    "            best_speed_idx = valid_fps_data['fps'].idxmax()\n",
    "            best_speed_model = valid_fps_data.loc[best_speed_idx]\n",
    "            \n",
    "            recommendations['best_speed'] = {\n",
    "                'model': best_speed_model['model_variant'],\n",
    "                'fps': best_speed_model['fps'],\n",
    "                'map50': best_speed_model.get('map50', 'N/A'),\n",
    "                'reasoning': 'Highest FPS - best for real-time applications'\n",
    "            }\n",
    "    \n",
    "    # 3. Most Efficient (Best Parameter Efficiency)\n",
    "    if 'parameter_efficiency' in df.columns:\n",
    "        valid_param_eff_data = df.dropna(subset=['parameter_efficiency'])\n",
    "        if len(valid_param_eff_data) > 0:\n",
    "            best_param_eff_idx = valid_param_eff_data['parameter_efficiency'].idxmax()\n",
    "            best_param_eff_model = valid_param_eff_data.loc[best_param_eff_idx]\n",
    "            \n",
    "            recommendations['most_efficient'] = {\n",
    "                'model': best_param_eff_model['model_variant'],\n",
    "                'parameter_efficiency': best_param_eff_model['parameter_efficiency'],\n",
    "                'map50': best_param_eff_model.get('map50', 'N/A'),\n",
    "                'reasoning': 'Best performance per parameter - optimal for resource-constrained environments'\n",
    "            }\n",
    "    \n",
    "    # 4. Best Edge Deployment Model\n",
    "    edge_suitable = filter_edge_suitable_models(df)\n",
    "    if len(edge_suitable) > 0 and 'map50' in edge_suitable.columns:\n",
    "        best_edge_idx = edge_suitable['map50'].idxmax()\n",
    "        best_edge_model = edge_suitable.loc[best_edge_idx]\n",
    "        \n",
    "        recommendations['best_edge'] = {\n",
    "            'model': best_edge_model['model_variant'],\n",
    "            'map50': best_edge_model['map50'],\n",
    "            'fps': best_edge_model.get('fps', 'N/A'),\n",
    "            'model_size_mb': best_edge_model.get('model_size_mb', 'N/A'),\n",
    "            'reasoning': 'Best performance while meeting all edge deployment constraints'\n",
    "        }\n",
    "    \n",
    "    # 5. Best Balanced Model (Pareto Optimal)\n",
    "    if 'fps' in df.columns and 'map50' in df.columns:\n",
    "        valid_data = df.dropna(subset=['fps', 'map50'])\n",
    "        if len(valid_data) > 0:\n",
    "            pareto_points = identify_pareto_optimal(valid_data, 'fps', 'map50')\n",
    "            \n",
    "            if len(pareto_points) > 0:\n",
    "                # Find the Pareto point closest to the \"ideal\" point (max FPS, max mAP)\n",
    "                max_fps = pareto_points['fps'].max()\n",
    "                max_map = pareto_points['map50'].max()\n",
    "                \n",
    "                # Calculate distance to ideal point (normalized)\n",
    "                pareto_points = pareto_points.copy()\n",
    "                pareto_points['distance_to_ideal'] = np.sqrt(\n",
    "                    ((pareto_points['fps'] - max_fps) / max_fps) ** 2 + \n",
    "                    ((pareto_points['map50'] - max_map) / max_map) ** 2\n",
    "                )\n",
    "                \n",
    "                best_balanced_idx = pareto_points['distance_to_ideal'].idxmin()\n",
    "                best_balanced_model = pareto_points.loc[best_balanced_idx]\n",
    "                \n",
    "                recommendations['best_balanced'] = {\n",
    "                    'model': best_balanced_model['model_variant'],\n",
    "                    'map50': best_balanced_model['map50'],\n",
    "                    'fps': best_balanced_model['fps'],\n",
    "                    'reasoning': 'Pareto optimal - best balance between performance and speed'\n",
    "                }\n",
    "    \n",
    "    # Print recommendations\n",
    "    for scenario, rec in recommendations.items():\n",
    "        print(f\"\\nðŸ† {scenario.replace('_', ' ').title()}:\")\n",
    "        print(f\"   Model: {rec['model']}\")\n",
    "        for key, value in rec.items():\n",
    "            if key not in ['model', 'reasoning']:\n",
    "                if isinstance(value, float):\n",
    "                    print(f\"   {key}: {value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"   {key}: {value}\")\n",
    "        print(f\"   ðŸ’¡ {rec['reasoning']}\")\n",
    "    \n",
    "    # Create deployment scenarios summary\n",
    "    print(f\"\\nðŸ“‹ DEPLOYMENT SCENARIOS SUMMARY:\")\n",
    "    print(f\"   ðŸ­ Manufacturing Line (High Accuracy): {recommendations.get('best_performance', {}).get('model', 'N/A')}\")\n",
    "    print(f\"   ðŸš€ Real-time Inspection (High Speed): {recommendations.get('best_speed', {}).get('model', 'N/A')}\")\n",
    "    print(f\"   ðŸ“± Edge Device (Resource Constrained): {recommendations.get('best_edge', {}).get('model', 'N/A')}\")\n",
    "    print(f\"   âš–ï¸  Balanced Application: {recommendations.get('best_balanced', {}).get('model', 'N/A')}\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = recommend_models_for_deployment(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_analysis(df):\n",
    "    \"\"\"\n",
    "    Perform statistical significance testing.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“ˆ STATISTICAL SIGNIFICANCE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        from scipy.stats import kruskal, mannwhitneyu, ttest_ind\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  SciPy not available - skipping statistical tests\")\n",
    "        return\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"âš ï¸  No data available for statistical analysis\")\n",
    "        return\n",
    "    \n",
    "    # 1. Model Type Comparison\n",
    "    if 'model_type' in df.columns and 'map50' in df.columns:\n",
    "        print(\"\\nðŸ”¬ Model Type Performance Comparison:\")\n",
    "        \n",
    "        valid_data = df.dropna(subset=['model_type', 'map50'])\n",
    "        model_groups = []\n",
    "        model_names = []\n",
    "        \n",
    "        for model_type in valid_data['model_type'].unique():\n",
    "            group_data = valid_data[valid_data['model_type'] == model_type]['map50']\n",
    "            if len(group_data) > 0:\n",
    "                model_groups.append(group_data)\n",
    "                model_names.append(model_type)\n",
    "        \n",
    "        if len(model_groups) >= 2:\n",
    "            if len(model_groups) > 2:\n",
    "                stat, p_value = kruskal(*model_groups)\n",
    "                test_name = \"Kruskal-Wallis\"\n",
    "            else:\n",
    "                stat, p_value = mannwhitneyu(model_groups[0], model_groups[1], alternative='two-sided')\n",
    "                test_name = \"Mann-Whitney U\"\n",
    "            \n",
    "            print(f\"   Test: {test_name}\")\n",
    "            print(f\"   Groups: {model_names}\")\n",
    "            print(f\"   Statistic: {stat:.4f}\")\n",
    "            print(f\"   P-value: {p_value:.4f}\")\n",
    "            print(f\"   Significant (Î±=0.05): {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                print(f\"   âœ… Significant difference detected between model types\")\n",
    "            else:\n",
    "                print(f\"   âž¡ï¸  No significant difference between model types\")\n",
    "    \n",
    "    # 2. Attention Mechanism Comparison\n",
    "    if 'attention_mechanism' in df.columns and 'map50' in df.columns:\n",
    "        print(\"\\nðŸŽ¯ Attention Mechanism Performance Comparison:\")\n",
    "        \n",
    "        valid_data = df.dropna(subset=['attention_mechanism', 'map50'])\n",
    "        \n",
    "        # Compare attention mechanisms vs baseline (none)\n",
    "        baseline_data = valid_data[valid_data['attention_mechanism'] == 'none']['map50']\n",
    "        attention_data = valid_data[valid_data['attention_mechanism'] != 'none']['map50']\n",
    "        \n",
    "        if len(baseline_data) > 0 and len(attention_data) > 0:\n",
    "            stat, p_value = mannwhitneyu(baseline_data, attention_data, alternative='two-sided')\n",
    "            \n",
    "            print(f\"   Test: Mann-Whitney U (Baseline vs Attention)\")\n",
    "            print(f\"   Baseline samples: {len(baseline_data)} (mean: {baseline_data.mean():.4f})\")\n",
    "            print(f\"   Attention samples: {len(attention_data)} (mean: {attention_data.mean():.4f})\")\n",
    "            print(f\"   Statistic: {stat:.4f}\")\n",
    "            print(f\"   P-value: {p_value:.4f}\")\n",
    "            print(f\"   Significant (Î±=0.05): {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "            \n",
    "            improvement = attention_data.mean() - baseline_data.mean()\n",
    "            if p_value < 0.05:\n",
    "                print(f\"   âœ… Attention mechanisms {'improve' if improvement > 0 else 'reduce'} performance by {abs(improvement):.4f} mAP@0.5\")\n",
    "            else:\n",
    "                print(f\"   âž¡ï¸  No significant difference from attention mechanisms\")\n",
    "    \n",
    "    # 3. Resolution Study Analysis\n",
    "    if 'image_size' in df.columns and 'map50' in df.columns:\n",
    "        print(\"\\nðŸ“ Resolution Impact Analysis:\")\n",
    "        \n",
    "        valid_data = df.dropna(subset=['image_size', 'map50'])\n",
    "        \n",
    "        # Compare standard resolution (640) vs high resolution\n",
    "        standard_res_data = valid_data[valid_data['image_size'] == 640]['map50']\n",
    "        high_res_data = valid_data[valid_data['image_size'] > 640]['map50']\n",
    "        \n",
    "        if len(standard_res_data) > 0 and len(high_res_data) > 0:\n",
    "            stat, p_value = mannwhitneyu(standard_res_data, high_res_data, alternative='two-sided')\n",
    "            \n",
    "            print(f\"   Test: Mann-Whitney U (640px vs High Resolution)\")\n",
    "            print(f\"   Standard res samples: {len(standard_res_data)} (mean: {standard_res_data.mean():.4f})\")\n",
    "            print(f\"   High res samples: {len(high_res_data)} (mean: {high_res_data.mean():.4f})\")\n",
    "            print(f\"   Statistic: {stat:.4f}\")\n",
    "            print(f\"   P-value: {p_value:.4f}\")\n",
    "            print(f\"   Significant (Î±=0.05): {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "            \n",
    "            improvement = high_res_data.mean() - standard_res_data.mean()\n",
    "            if p_value < 0.05:\n",
    "                print(f\"   âœ… High resolution {'improves' if improvement > 0 else 'reduces'} performance by {abs(improvement):.4f} mAP@0.5\")\n",
    "            else:\n",
    "                print(f\"   âž¡ï¸  No significant difference from higher resolution\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Statistical analysis complete.\")\n",
    "\n",
    "\n",
    "# Perform statistical analysis\n",
    "perform_statistical_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_analysis_results(df, recommendations):\n",
    "    \"\"\"\n",
    "    Export comprehensive analysis results.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ’¾ Exporting analysis results...\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"âš ï¸  No data to export\")\n",
    "        return\n",
    "    \n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. Export complete results to CSV\n",
    "    csv_path = OUTPUT_DIR / f\"complete_results_{timestamp}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"   ðŸ“„ Complete results: {csv_path}\")\n",
    "    \n",
    "    # 2. Export to Excel with multiple sheets\n",
    "    try:\n",
    "        excel_path = OUTPUT_DIR / f\"pcb_defect_analysis_{timestamp}.xlsx\"\n",
    "        \n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "            # Main results\n",
    "            df.to_excel(writer, sheet_name='Complete_Results', index=False)\n",
    "            \n",
    "            # Summary statistics\n",
    "            if 'map50' in df.columns:\n",
    "                summary_stats = df.groupby('model_variant').agg({\n",
    "                    'map50': ['count', 'mean', 'std', 'min', 'max'],\n",
    "                    'map50_95': ['mean', 'std'],\n",
    "                    'training_time_hours': ['mean', 'std'],\n",
    "                    'total_parameters': 'first'\n",
    "                }).round(4)\n",
    "                \n",
    "                summary_stats.to_excel(writer, sheet_name='Summary_Statistics')\n",
    "            \n",
    "            # Model recommendations\n",
    "            if recommendations:\n",
    "                rec_df = pd.DataFrame.from_dict(recommendations, orient='index')\n",
    "                rec_df.to_excel(writer, sheet_name='Recommendations')\n",
    "            \n",
    "            # Pareto optimal models\n",
    "            if 'fps' in df.columns and 'map50' in df.columns:\n",
    "                valid_data = df.dropna(subset=['fps', 'map50'])\n",
    "                if len(valid_data) > 0:\n",
    "                    pareto_points = identify_pareto_optimal(valid_data, 'fps', 'map50')\n",
    "                    if len(pareto_points) > 0:\n",
    "                        pareto_points.to_excel(writer, sheet_name='Pareto_Optimal', index=False)\n",
    "            \n",
    "            # Edge suitable models\n",
    "            edge_suitable = filter_edge_suitable_models(df)\n",
    "            if len(edge_suitable) > 0:\n",
    "                edge_suitable.to_excel(writer, sheet_name='Edge_Suitable', index=False)\n",
    "        \n",
    "        print(f\"   ðŸ“Š Excel analysis: {excel_path}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"   âš ï¸  openpyxl not available - skipping Excel export\")\n",
    "    \n",
    "    # 3. Generate comprehensive text report\n",
    "    report_path = OUTPUT_DIR / f\"analysis_report_{timestamp}.txt\"\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"PCB DEFECT DETECTION: SYSTEMATIC STUDY ANALYSIS REPORT\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"WandB Project: {WANDB_PROJECT}\\n\\n\")\n",
    "        \n",
    "        # Executive Summary\n",
    "        f.write(\"EXECUTIVE SUMMARY\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        f.write(f\"Total Experiments: {len(df)}\\n\")\n",
    "        \n",
    "        if 'map50' in df.columns:\n",
    "            f.write(f\"Average mAP@0.5: {df['map50'].mean():.4f} Â± {df['map50'].std():.4f}\\n\")\n",
    "            f.write(f\"Best Performance: {df['map50'].max():.4f} ({df.loc[df['map50'].idxmax(), 'model_variant']})\\n\")\n",
    "        \n",
    "        if 'training_time_hours' in df.columns:\n",
    "            f.write(f\"Total Training Time: {df['training_time_hours'].sum():.1f} hours\\n\")\n",
    "        \n",
    "        # Model Recommendations\n",
    "        if recommendations:\n",
    "            f.write(\"\\nMODEL RECOMMENDATIONS\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            for scenario, rec in recommendations.items():\n",
    "                f.write(f\"{scenario.replace('_', ' ').title()}: {rec['model']}\\n\")\n",
    "                f.write(f\"  {rec['reasoning']}\\n\\n\")\n",
    "        \n",
    "        # Deployment Guidelines\n",
    "        f.write(\"DEPLOYMENT GUIDELINES\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        f.write(\"â€¢ Manufacturing Line (High Accuracy): Use best performing model\\n\")\n",
    "        f.write(\"â€¢ Real-time Inspection (High Speed): Prioritize FPS over accuracy\\n\")\n",
    "        f.write(\"â€¢ Edge Devices (Resource Constrained): Consider model size and inference time\\n\")\n",
    "        f.write(\"â€¢ Balanced Applications: Use Pareto optimal models\\n\\n\")\n",
    "        \n",
    "        f.write(\"Analysis generated by PCB Defect Detection Analysis Notebook\\n\")\n",
    "    \n",
    "    print(f\"   ðŸ“ Text report: {report_path}\")\n",
    "    \n",
    "    print(f\"\\nâœ… All analysis results exported to: {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "# Export results\n",
    "if 'recommendations' in locals():\n",
    "    export_analysis_results(df, recommendations)\n",
    "else:\n",
    "    export_analysis_results(df, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_summary(df):\n",
    "    \"\"\"\n",
    "    Generate final summary and conclusions.\n",
    "    \"\"\"\n",
    "    print(\"ðŸŽ‰ FINAL ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"âš ï¸  No data available for final summary\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nðŸ“Š **Experiment Overview:**\")\n",
    "    print(f\"   â€¢ Total completed experiments: {len(df)}\")\n",
    "    print(f\"   â€¢ Model variants tested: {df['model_variant'].nunique()}\")\n",
    "    print(f\"   â€¢ Attention mechanisms: {df['attention_mechanism'].nunique()}\")\n",
    "    \n",
    "    if 'training_time_hours' in df.columns:\n",
    "        print(f\"   â€¢ Total training investment: {df['training_time_hours'].sum():.1f} hours\")\n",
    "    \n",
    "    # Performance insights\n",
    "    if 'map50' in df.columns:\n",
    "        best_model_idx = df['map50'].idxmax()\n",
    "        best_model = df.loc[best_model_idx]\n",
    "        \n",
    "        print(f\"\\nðŸ† **Key Performance Insights:**\")\n",
    "        print(f\"   â€¢ Best overall performance: {best_model['model_variant']} ({best_model['map50']:.4f} mAP@0.5)\")\n",
    "        print(f\"   â€¢ Performance range: [{df['map50'].min():.4f}, {df['map50'].max():.4f}] mAP@0.5\")\n",
    "        print(f\"   â€¢ Average performance: {df['map50'].mean():.4f} Â± {df['map50'].std():.4f}\")\n",
    "    \n",
    "    # Efficiency insights\n",
    "    edge_suitable = filter_edge_suitable_models(df)\n",
    "    print(f\"\\nâš¡ **Efficiency Insights:**\")\n",
    "    print(f\"   â€¢ Models suitable for edge deployment: {len(edge_suitable)}/{len(df)}\")\n",
    "    \n",
    "    if 'fps' in df.columns:\n",
    "        valid_fps = df.dropna(subset=['fps'])\n",
    "        if len(valid_fps) > 0:\n",
    "            fastest_model_idx = valid_fps['fps'].idxmax()\n",
    "            fastest_model = valid_fps.loc[fastest_model_idx]\n",
    "            print(f\"   â€¢ Fastest inference: {fastest_model['model_variant']} ({fastest_model['fps']:.1f} FPS)\")\n",
    "    \n",
    "    if 'parameter_efficiency' in df.columns:\n",
    "        valid_param_eff = df.dropna(subset=['parameter_efficiency'])\n",
    "        if len(valid_param_eff) > 0:\n",
    "            most_efficient_idx = valid_param_eff['parameter_efficiency'].idxmax()\n",
    "            most_efficient = valid_param_eff.loc[most_efficient_idx]\n",
    "            print(f\"   â€¢ Most parameter efficient: {most_efficient['model_variant']} ({most_efficient['parameter_efficiency']:.4f} mAP/M params)\")\n",
    "    \n",
    "    # Research insights\n",
    "    print(f\"\\nðŸ”¬ **Research Insights:**\")\n",
    "    \n",
    "    # Attention mechanism effectiveness\n",
    "    if 'attention_mechanism' in df.columns and 'map50' in df.columns:\n",
    "        baseline_performance = df[df['attention_mechanism'] == 'none']['map50'].mean()\n",
    "        attention_performance = df[df['attention_mechanism'] != 'none']['map50'].mean()\n",
    "        \n",
    "        if not pd.isna(baseline_performance) and not pd.isna(attention_performance):\n",
    "            improvement = attention_performance - baseline_performance\n",
    "            print(f\"   â€¢ Attention mechanisms {'improve' if improvement > 0 else 'reduce'} performance by {abs(improvement):.4f} mAP@0.5 on average\")\n",
    "    \n",
    "    # Resolution impact\n",
    "    if 'image_size' in df.columns and 'map50' in df.columns:\n",
    "        standard_res_perf = df[df['image_size'] == 640]['map50'].mean()\n",
    "        high_res_perf = df[df['image_size'] > 640]['map50'].mean()\n",
    "        \n",
    "        if not pd.isna(standard_res_perf) and not pd.isna(high_res_perf):\n",
    "            improvement = high_res_perf - standard_res_perf\n",
    "            print(f\"   â€¢ High resolution {'improves' if improvement > 0 else 'reduces'} performance by {abs(improvement):.4f} mAP@0.5 on average\")\n",
    "    \n",
    "    # Model architecture comparison\n",
    "    if 'model_type' in df.columns and 'map50' in df.columns:\n",
    "        model_performance = df.groupby('model_type')['map50'].mean().sort_values(ascending=False)\n",
    "        if len(model_performance) > 1:\n",
    "            print(f\"   â€¢ Best architecture: {model_performance.index[0]} ({model_performance.iloc[0]:.4f} mAP@0.5)\")\n",
    "    \n",
    "    # Final recommendations\n",
    "    print(f\"\\nðŸŽ¯ **Final Recommendations:**\")\n",
    "    \n",
    "    if 'map50' in df.columns:\n",
    "        # Production deployment\n",
    "        if df['map50'].max() > 0.8:\n",
    "            print(f\"   âœ… Ready for production deployment (mAP@0.5 > 0.8 achieved)\")\n",
    "        elif df['map50'].max() > 0.6:\n",
    "            print(f\"   ðŸ‘ Good performance achieved, suitable for most applications\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  Consider additional optimization strategies\")\n",
    "    \n",
    "    print(f\"   â€¢ Implement model versioning and A/B testing in production\")\n",
    "    print(f\"   â€¢ Set up continuous monitoring of model performance\")\n",
    "    print(f\"   â€¢ Consider ensemble methods for critical applications\")\n",
    "    print(f\"   â€¢ Establish retraining pipelines for model updates\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ **Generated Outputs:**\")\n",
    "    print(f\"   â€¢ Comprehensive comparison table (CSV/Excel)\")\n",
    "    print(f\"   â€¢ Performance visualization plots\")\n",
    "    print(f\"   â€¢ Pareto analysis for trade-off optimization\")\n",
    "    print(f\"   â€¢ Interactive plots (if Plotly available)\")\n",
    "    print(f\"   â€¢ Statistical significance analysis\")\n",
    "    print(f\"   â€¢ Model recommendations for different scenarios\")\n",
    "    print(f\"   â€¢ Edge deployment suitability analysis\")\n",
    "    \n",
    "    print(f\"\\nðŸš€ **Analysis Complete!**\")\n",
    "    print(f\"All results saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "# Generate final summary\n",
    "generate_final_summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“‹ Analysis Checklist\n",
    "\n",
    "This notebook has completed the following analysis tasks:\n",
    "\n",
    "### âœ… **Data Acquisition & Processing**\n",
    "- [x] Fetch all experimental runs from WandB API\n",
    "- [x] Extract comprehensive metrics (mAP@0.5, mAP@0.5-0.95, precision, recall, F1)\n",
    "- [x] Parse model configurations (type, attention mechanism, loss function, image size)\n",
    "- [x] Clean and preprocess data for analysis\n",
    "\n",
    "### âœ… **Performance Analysis** \n",
    "- [x] Generate summary statistics for all metrics\n",
    "- [x] Compare model types (YOLOv8n, YOLOv8s, YOLOv10s)\n",
    "- [x] Evaluate attention mechanism effectiveness\n",
    "- [x] Analyze resolution impact on performance\n",
    "- [x] Calculate training and parameter efficiency\n",
    "\n",
    "### âœ… **Trade-off Analysis**\n",
    "- [x] **Pareto analysis for mAP vs FPS optimization**\n",
    "- [x] **Performance vs model size trade-offs**\n",
    "- [x] Training time vs accuracy analysis\n",
    "- [x] Parameter efficiency evaluation\n",
    "- [x] 3D visualization of performance-speed-size relationships\n",
    "\n",
    "### âœ… **Edge Deployment Analysis**\n",
    "- [x] Define edge deployment constraints\n",
    "- [x] Filter models suitable for edge deployment\n",
    "- [x] Identify Pareto optimal solutions\n",
    "- [x] **Generate deployment recommendations**\n",
    "\n",
    "### âœ… **Statistical Validation**\n",
    "- [x] Significance testing for model comparisons\n",
    "- [x] Attention mechanism effectiveness validation\n",
    "- [x] Resolution impact statistical analysis\n",
    "\n",
    "### âœ… **Visualization & Reporting**\n",
    "- [x] Performance comparison plots\n",
    "- [x] **Pareto frontier visualizations**\n",
    "- [x] Correlation analysis heatmaps\n",
    "- [x] Interactive 3D plots (if Plotly available)\n",
    "- [x] Edge deployment suitability charts\n",
    "\n",
    "### âœ… **Export & Documentation**\n",
    "- [x] **Comprehensive comparison table (CSV/Excel)**\n",
    "- [x] Model recommendations for different scenarios\n",
    "- [x] Statistical analysis results\n",
    "- [x] Deployment guidelines\n",
    "- [x] Complete analysis report\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Key Deliverables\n",
    "\n",
    "1. **ðŸ“Š Comprehensive DataFrame** with all experimental results organized for easy comparison\n",
    "2. **ðŸ“ˆ Pareto Plots** showing optimal trade-offs between mAP@0.5 and FPS for edge deployment\n",
    "3. **ðŸŽ¯ Model Recommendations** for different deployment scenarios (accuracy-critical, real-time, edge, balanced)\n",
    "4. **ðŸ“‹ Deployment Guidelines** with specific model suggestions for each use case\n",
    "5. **ðŸ“Š Interactive Visualizations** for exploring model performance trade-offs\n",
    "6. **ðŸ“„ Exportable Results** in multiple formats (CSV, Excel, HTML) for further analysis\n",
    "\n",
    "This analysis provides everything needed to select the optimal YOLOv8 configuration for your specific PCB defect detection deployment requirements!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}